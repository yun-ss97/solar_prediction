{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn import ensemble\n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import scipy as sp\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "train_dir = os.path.join(base_dir, 'train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.08</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.06</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.78</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.75</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>75.20</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Hour  Minute  DHI  DNI   WS     RH   T  TARGET\n",
       "0    0     0       0    0    0  1.5  69.08 -12     0.0\n",
       "1    0     0      30    0    0  1.5  69.06 -12     0.0\n",
       "2    0     1       0    0    0  1.6  71.78 -12     0.0\n",
       "3    0     1      30    0    0  1.6  71.75 -12     0.0\n",
       "4    0     2       0    0    0  1.6  75.20 -12     0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df = pd.read_csv(train_dir)\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df['Time'] = base_df['Hour']*60 + base_df['Minute']\n",
    "\n",
    "dataset = base_df[['Day', 'Time', 'DHI', 'DNI', 'WS', 'RH', 'T', 'TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.08</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.06</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.78</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.75</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>75.20</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52555</th>\n",
       "      <td>1094</td>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>70.70</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52556</th>\n",
       "      <td>1094</td>\n",
       "      <td>1320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>66.79</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52557</th>\n",
       "      <td>1094</td>\n",
       "      <td>1350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>66.78</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52558</th>\n",
       "      <td>1094</td>\n",
       "      <td>1380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>67.72</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52559</th>\n",
       "      <td>1094</td>\n",
       "      <td>1410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>67.70</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52560 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Day  Time  DHI  DNI   WS     RH   T  TARGET\n",
       "0         0     0    0    0  1.5  69.08 -12     0.0\n",
       "1         0    30    0    0  1.5  69.06 -12     0.0\n",
       "2         0    60    0    0  1.6  71.78 -12     0.0\n",
       "3         0    90    0    0  1.6  71.75 -12     0.0\n",
       "4         0   120    0    0  1.6  75.20 -12     0.0\n",
       "...     ...   ...  ...  ...  ...    ...  ..     ...\n",
       "52555  1094  1290    0    0  2.4  70.70  -4     0.0\n",
       "52556  1094  1320    0    0  2.4  66.79  -4     0.0\n",
       "52557  1094  1350    0    0  2.2  66.78  -4     0.0\n",
       "52558  1094  1380    0    0  2.1  67.72  -4     0.0\n",
       "52559  1094  1410    0    0  2.1  67.70  -4     0.0\n",
       "\n",
       "[52560 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaler -- standard or minmax? 0~1?\n",
    "scaleset = dataset.drop('Day', axis=1).values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(scaleset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_blocks = []\n",
    "for i in range(0, 1094-3):\n",
    "    temp_block = dataset.loc[(dataset.Day >= i) & (dataset.Day <= i + 4)]\n",
    "    temp_block = temp_block.drop('Day', axis = 1)\n",
    "    all_blocks.append(temp_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val blocks\n",
    "import random\n",
    "\n",
    "def pick_one_batch(train_num, val_num, base_block):\n",
    "    \n",
    "    train_num = train_num\n",
    "    val_num = val_num\n",
    "    all_blocks = base_block\n",
    "\n",
    "    train_batch_idx = [random.choice(range(len(all_blocks))) for i in range(train_num)]\n",
    "    train_batch = [all_blocks[i] for i in train_batch_idx]\n",
    "\n",
    "    val_batch_cand = [k for i, k in enumerate(all_blocks) if i not in train_batch_idx]\n",
    "    val_batch = [random.choice(val_batch_cand) for i in range(val_num)]\n",
    "\n",
    "    train_x = torch.tensor(np.array([scaler.transform(batch.values)[:144, :] for batch in train_batch])).float()\n",
    "    train_x = train_x.permute(0, 2, 1)\n",
    "    train_y = torch.tensor(np.array([batch.values[144:, -1] for batch in train_batch])).float()\n",
    "\n",
    "    val_x = torch.tensor(np.array([scaler.transform(batch.values)[:144, :] for batch in val_batch])).float()\n",
    "    val_x = val_x.permute(0, 2, 1)\n",
    "    val_y = torch.tensor(np.array([batch.values[144:, -1] for batch in val_batch])).float()\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaconCNN(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(DaconCNN, self).__init__()\n",
    "        \n",
    "        # block1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=feature_dim, out_channels=32, kernel_size=6),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=6),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(64*134, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 96),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.block1(x)\n",
    "        #print(out.size())\n",
    "        batch_size = out.size()[0]\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.block2(out)\n",
    "        \n",
    "        out = torch.max(torch.zeros_like(out), out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuantileLoss(qs, pred, gt):\n",
    "    #qs = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    qs = qs if isinstance(qs, list) else [qs]\n",
    "    \n",
    "    sum_loss = 0\n",
    "    for i, q in enumerate(qs):\n",
    "        loss = pred - gt\n",
    "        loss = torch.max(q*loss, (q-1)*loss)\n",
    "        sum_loss += torch.mean(loss)\n",
    "        \n",
    "    fin_loss = sum_loss/len(qs)# + 0.05*symloss/4\n",
    "    return fin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def trainCNN(epochs, base_block, qs):\n",
    "    \n",
    "    train_num = 32\n",
    "    val_num = 16\n",
    "    qs = qs\n",
    "    \n",
    "    model = DaconCNN(feature_dim=7).float()\n",
    "    \n",
    "    learning_rate = 0.005\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    decay_rate = 0.998\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer,\n",
    "                                                         gamma=decay_rate)\n",
    "    \n",
    "    train_loss_sum = 0.0\n",
    "    val_loss_sum = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for i in range(30):\n",
    "            \n",
    "            train_x, train_y, val_x, val_y = pick_one_batch(train_num, val_num, base_block)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_pred = model(train_x)\n",
    "            train_loss = QuantileLoss(qs, train_pred, train_y)\n",
    "            \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_sum += train_loss\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            val_pred = model(val_x)\n",
    "            val_loss = QuantileLoss(qs, val_pred, val_y)\n",
    "            val_loss_sum += val_loss\n",
    "                \n",
    "        print(f\"Epoch: {epoch+1} | Loss: {train_loss_sum.item()/30:.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "        \n",
    "        train_loss_sum, val_loss_sum = 0.0, 0.0\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "    return model\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([16, 96]) torch.Size([16, 96])\n",
      "Epoch: 1 | Loss: 1.5971 | Val Loss: 1.1651\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-262-1d710dc5a633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_fit_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-261-c47c973e1dfc>\u001b[0m in \u001b[0;36mtrainCNN\u001b[0;34m(epochs, base_block, qs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtrain_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dacon/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dacon/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dacon/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dacon/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "model_fit_4 = trainCNN(epochs, all_blocks, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 1.4128 | Val Loss: 1.2064\n",
      "Epoch: 2 | Loss: 1.2847 | Val Loss: 1.2453\n",
      "Epoch: 3 | Loss: 1.1862 | Val Loss: 1.4324\n",
      "Epoch: 4 | Loss: 1.0724 | Val Loss: 1.2983\n",
      "Epoch: 5 | Loss: 0.9590 | Val Loss: 1.0381\n",
      "Epoch: 6 | Loss: 0.8556 | Val Loss: 0.8214\n",
      "Epoch: 7 | Loss: 0.7700 | Val Loss: 0.8628\n",
      "Epoch: 8 | Loss: 0.6728 | Val Loss: 0.6803\n",
      "Epoch: 9 | Loss: 0.6119 | Val Loss: 0.6925\n",
      "Epoch: 10 | Loss: 0.5576 | Val Loss: 0.7278\n",
      "Epoch: 11 | Loss: 0.5197 | Val Loss: 0.6069\n",
      "Epoch: 12 | Loss: 0.4956 | Val Loss: 0.7399\n",
      "Epoch: 13 | Loss: 0.4701 | Val Loss: 0.5235\n",
      "Epoch: 14 | Loss: 0.4416 | Val Loss: 0.4930\n",
      "Epoch: 15 | Loss: 0.4332 | Val Loss: 0.5391\n",
      "Epoch: 16 | Loss: 0.4135 | Val Loss: 0.4071\n",
      "Epoch: 17 | Loss: 0.4082 | Val Loss: 0.4065\n",
      "Epoch: 18 | Loss: 0.3856 | Val Loss: 0.5517\n",
      "Epoch: 19 | Loss: 0.3889 | Val Loss: 0.4522\n",
      "Epoch: 20 | Loss: 0.3656 | Val Loss: 0.4487\n",
      "Epoch: 21 | Loss: 0.3564 | Val Loss: 0.5358\n",
      "Epoch: 22 | Loss: 0.3457 | Val Loss: 0.4015\n",
      "Epoch: 23 | Loss: 0.3447 | Val Loss: 0.6300\n",
      "Epoch: 24 | Loss: 0.3333 | Val Loss: 0.4163\n",
      "Epoch: 25 | Loss: 0.3257 | Val Loss: 0.4966\n",
      "Epoch: 26 | Loss: 0.3303 | Val Loss: 0.4788\n",
      "Epoch: 27 | Loss: 0.3123 | Val Loss: 0.4673\n",
      "Epoch: 28 | Loss: 0.3213 | Val Loss: 0.3981\n",
      "Epoch: 29 | Loss: 0.3004 | Val Loss: 0.3898\n",
      "Epoch: 30 | Loss: 0.2997 | Val Loss: 0.4156\n",
      "Epoch: 31 | Loss: 0.2961 | Val Loss: 0.6497\n",
      "Epoch: 32 | Loss: 0.3011 | Val Loss: 0.4856\n",
      "Epoch: 33 | Loss: 0.2878 | Val Loss: 0.6640\n",
      "Epoch: 34 | Loss: 0.2863 | Val Loss: 0.5563\n",
      "Epoch: 35 | Loss: 0.2834 | Val Loss: 0.3762\n",
      "Epoch: 36 | Loss: 0.2717 | Val Loss: 0.4499\n",
      "Epoch: 37 | Loss: 0.2806 | Val Loss: 0.3545\n",
      "Epoch: 38 | Loss: 0.2811 | Val Loss: 0.3782\n",
      "Epoch: 39 | Loss: 0.2786 | Val Loss: 0.4163\n",
      "Epoch: 40 | Loss: 0.2679 | Val Loss: 0.4030\n",
      "Epoch: 1 | Loss: 2.4673 | Val Loss: 2.0745\n",
      "Epoch: 2 | Loss: 2.0812 | Val Loss: 2.0094\n",
      "Epoch: 3 | Loss: 1.9329 | Val Loss: 1.5619\n",
      "Epoch: 4 | Loss: 1.7787 | Val Loss: 1.8259\n",
      "Epoch: 5 | Loss: 1.5870 | Val Loss: 1.5487\n",
      "Epoch: 6 | Loss: 1.4065 | Val Loss: 1.4102\n",
      "Epoch: 7 | Loss: 1.2495 | Val Loss: 1.7396\n",
      "Epoch: 8 | Loss: 1.1070 | Val Loss: 1.4620\n",
      "Epoch: 9 | Loss: 1.0184 | Val Loss: 1.0062\n",
      "Epoch: 10 | Loss: 0.9125 | Val Loss: 1.1740\n",
      "Epoch: 11 | Loss: 0.8638 | Val Loss: 0.9573\n",
      "Epoch: 12 | Loss: 0.7938 | Val Loss: 0.8575\n",
      "Epoch: 13 | Loss: 0.7649 | Val Loss: 0.9244\n",
      "Epoch: 14 | Loss: 0.7079 | Val Loss: 0.8970\n",
      "Epoch: 15 | Loss: 0.6578 | Val Loss: 0.9747\n",
      "Epoch: 16 | Loss: 0.6333 | Val Loss: 0.8224\n",
      "Epoch: 17 | Loss: 0.6069 | Val Loss: 0.9642\n",
      "Epoch: 18 | Loss: 0.5781 | Val Loss: 0.5906\n",
      "Epoch: 19 | Loss: 0.5733 | Val Loss: 0.7297\n",
      "Epoch: 20 | Loss: 0.5526 | Val Loss: 0.6518\n",
      "Epoch: 21 | Loss: 0.5231 | Val Loss: 0.6163\n",
      "Epoch: 22 | Loss: 0.5236 | Val Loss: 0.8244\n",
      "Epoch: 23 | Loss: 0.4984 | Val Loss: 0.5499\n",
      "Epoch: 24 | Loss: 0.4770 | Val Loss: 0.6475\n",
      "Epoch: 25 | Loss: 0.4799 | Val Loss: 0.6272\n",
      "Epoch: 26 | Loss: 0.4737 | Val Loss: 0.5268\n",
      "Epoch: 27 | Loss: 0.4656 | Val Loss: 0.5127\n",
      "Epoch: 28 | Loss: 0.4644 | Val Loss: 0.5376\n",
      "Epoch: 29 | Loss: 0.4564 | Val Loss: 0.7331\n",
      "Epoch: 30 | Loss: 0.4494 | Val Loss: 0.9156\n",
      "Epoch: 31 | Loss: 0.4333 | Val Loss: 0.5693\n",
      "Epoch: 32 | Loss: 0.4364 | Val Loss: 0.5418\n",
      "Epoch: 33 | Loss: 0.4253 | Val Loss: 0.6019\n",
      "Epoch: 34 | Loss: 0.4287 | Val Loss: 0.6166\n",
      "Epoch: 35 | Loss: 0.4228 | Val Loss: 0.7657\n",
      "Epoch: 36 | Loss: 0.4162 | Val Loss: 0.9885\n",
      "Epoch: 37 | Loss: 0.4088 | Val Loss: 0.6743\n",
      "Epoch: 38 | Loss: 0.4127 | Val Loss: 0.5685\n",
      "Epoch: 39 | Loss: 0.4025 | Val Loss: 0.6320\n",
      "Epoch: 40 | Loss: 0.4032 | Val Loss: 0.6688\n",
      "Epoch: 1 | Loss: 3.2769 | Val Loss: 2.7381\n",
      "Epoch: 2 | Loss: 2.5376 | Val Loss: 2.2499\n",
      "Epoch: 3 | Loss: 2.3668 | Val Loss: 2.3303\n",
      "Epoch: 4 | Loss: 2.1836 | Val Loss: 1.8550\n",
      "Epoch: 5 | Loss: 1.9761 | Val Loss: 1.8661\n",
      "Epoch: 6 | Loss: 1.7520 | Val Loss: 1.7053\n",
      "Epoch: 7 | Loss: 1.5814 | Val Loss: 1.3968\n",
      "Epoch: 8 | Loss: 1.4237 | Val Loss: 1.4213\n",
      "Epoch: 9 | Loss: 1.2788 | Val Loss: 1.2995\n",
      "Epoch: 10 | Loss: 1.1854 | Val Loss: 1.1284\n",
      "Epoch: 11 | Loss: 1.0787 | Val Loss: 1.0397\n",
      "Epoch: 12 | Loss: 1.0152 | Val Loss: 0.9914\n",
      "Epoch: 13 | Loss: 0.9481 | Val Loss: 0.8943\n",
      "Epoch: 14 | Loss: 0.8672 | Val Loss: 0.9944\n",
      "Epoch: 15 | Loss: 0.8303 | Val Loss: 0.9343\n",
      "Epoch: 16 | Loss: 0.7756 | Val Loss: 0.8544\n",
      "Epoch: 17 | Loss: 0.7516 | Val Loss: 0.8583\n",
      "Epoch: 18 | Loss: 0.7006 | Val Loss: 1.3704\n",
      "Epoch: 19 | Loss: 0.6809 | Val Loss: 0.8872\n",
      "Epoch: 20 | Loss: 0.6545 | Val Loss: 1.0321\n",
      "Epoch: 21 | Loss: 0.6280 | Val Loss: 0.8552\n",
      "Epoch: 22 | Loss: 0.5917 | Val Loss: 0.6810\n",
      "Epoch: 23 | Loss: 0.5896 | Val Loss: 0.8701\n",
      "Epoch: 24 | Loss: 0.5783 | Val Loss: 0.7527\n",
      "Epoch: 25 | Loss: 0.5609 | Val Loss: 0.6783\n",
      "Epoch: 26 | Loss: 0.5568 | Val Loss: 0.7495\n",
      "Epoch: 27 | Loss: 0.5346 | Val Loss: 0.8155\n",
      "Epoch: 28 | Loss: 0.5061 | Val Loss: 0.9026\n",
      "Epoch: 29 | Loss: 0.5255 | Val Loss: 0.8719\n",
      "Epoch: 30 | Loss: 0.5191 | Val Loss: 0.9785\n",
      "Epoch: 31 | Loss: 0.5098 | Val Loss: 0.6891\n",
      "Epoch: 32 | Loss: 0.4853 | Val Loss: 0.7910\n",
      "Epoch: 33 | Loss: 0.4880 | Val Loss: 1.3832\n",
      "Epoch: 34 | Loss: 0.4857 | Val Loss: 0.8994\n",
      "Epoch: 35 | Loss: 0.4707 | Val Loss: 0.6752\n",
      "Epoch: 36 | Loss: 0.4843 | Val Loss: 0.5867\n",
      "Epoch: 37 | Loss: 0.4576 | Val Loss: 0.6317\n",
      "Epoch: 38 | Loss: 0.4573 | Val Loss: 0.5839\n",
      "Epoch: 39 | Loss: 0.4735 | Val Loss: 0.8147\n",
      "Epoch: 40 | Loss: 0.4391 | Val Loss: 0.6529\n",
      "Epoch: 1 | Loss: 3.8805 | Val Loss: 2.7617\n",
      "Epoch: 2 | Loss: 2.7150 | Val Loss: 2.8730\n",
      "Epoch: 3 | Loss: 2.5282 | Val Loss: 2.3079\n",
      "Epoch: 4 | Loss: 2.3284 | Val Loss: 2.5742\n",
      "Epoch: 5 | Loss: 2.1936 | Val Loss: 1.9919\n",
      "Epoch: 6 | Loss: 1.9735 | Val Loss: 2.0509\n",
      "Epoch: 7 | Loss: 1.8235 | Val Loss: 1.9431\n",
      "Epoch: 8 | Loss: 1.6557 | Val Loss: 1.5960\n",
      "Epoch: 9 | Loss: 1.5360 | Val Loss: 1.4670\n",
      "Epoch: 10 | Loss: 1.4508 | Val Loss: 1.3589\n",
      "Epoch: 11 | Loss: 1.3462 | Val Loss: 1.4487\n",
      "Epoch: 12 | Loss: 1.2550 | Val Loss: 1.4546\n",
      "Epoch: 13 | Loss: 1.1671 | Val Loss: 1.6365\n",
      "Epoch: 14 | Loss: 1.1044 | Val Loss: 1.4826\n",
      "Epoch: 15 | Loss: 1.0337 | Val Loss: 1.1989\n",
      "Epoch: 16 | Loss: 0.9781 | Val Loss: 1.3563\n",
      "Epoch: 17 | Loss: 0.9181 | Val Loss: 1.1939\n",
      "Epoch: 18 | Loss: 0.8610 | Val Loss: 1.0695\n",
      "Epoch: 19 | Loss: 0.8184 | Val Loss: 0.7136\n",
      "Epoch: 20 | Loss: 0.7899 | Val Loss: 0.8315\n",
      "Epoch: 21 | Loss: 0.7310 | Val Loss: 1.2325\n",
      "Epoch: 22 | Loss: 0.7232 | Val Loss: 1.0210\n",
      "Epoch: 23 | Loss: 0.6943 | Val Loss: 0.7722\n",
      "Epoch: 24 | Loss: 0.6611 | Val Loss: 0.9011\n",
      "Epoch: 25 | Loss: 0.6473 | Val Loss: 0.8732\n",
      "Epoch: 26 | Loss: 0.6315 | Val Loss: 0.8532\n",
      "Epoch: 27 | Loss: 0.5972 | Val Loss: 1.0715\n",
      "Epoch: 28 | Loss: 0.5804 | Val Loss: 0.6570\n",
      "Epoch: 29 | Loss: 0.5707 | Val Loss: 0.9308\n",
      "Epoch: 30 | Loss: 0.5664 | Val Loss: 0.7332\n",
      "Epoch: 31 | Loss: 0.5469 | Val Loss: 0.6159\n",
      "Epoch: 32 | Loss: 0.5468 | Val Loss: 0.7782\n",
      "Epoch: 33 | Loss: 0.5318 | Val Loss: 0.6487\n",
      "Epoch: 34 | Loss: 0.5226 | Val Loss: 0.8939\n",
      "Epoch: 35 | Loss: 0.4958 | Val Loss: 0.5145\n",
      "Epoch: 36 | Loss: 0.5057 | Val Loss: 0.7194\n",
      "Epoch: 37 | Loss: 0.4961 | Val Loss: 0.9301\n",
      "Epoch: 38 | Loss: 0.5036 | Val Loss: 1.0379\n",
      "Epoch: 39 | Loss: 0.4998 | Val Loss: 0.9509\n",
      "Epoch: 40 | Loss: 0.4987 | Val Loss: 0.6528\n",
      "Epoch: 1 | Loss: 4.2943 | Val Loss: 2.6749\n",
      "Epoch: 2 | Loss: 2.6430 | Val Loss: 2.6406\n",
      "Epoch: 3 | Loss: 2.5069 | Val Loss: 3.3845\n",
      "Epoch: 4 | Loss: 2.3407 | Val Loss: 2.3654\n",
      "Epoch: 5 | Loss: 2.2367 | Val Loss: 2.3287\n",
      "Epoch: 6 | Loss: 2.0784 | Val Loss: 2.2661\n",
      "Epoch: 7 | Loss: 1.9429 | Val Loss: 2.0010\n",
      "Epoch: 8 | Loss: 1.7990 | Val Loss: 2.1500\n",
      "Epoch: 9 | Loss: 1.6724 | Val Loss: 1.8741\n",
      "Epoch: 10 | Loss: 1.5610 | Val Loss: 1.6910\n",
      "Epoch: 11 | Loss: 1.4723 | Val Loss: 1.3582\n",
      "Epoch: 12 | Loss: 1.3833 | Val Loss: 1.2903\n",
      "Epoch: 13 | Loss: 1.2994 | Val Loss: 1.4495\n",
      "Epoch: 14 | Loss: 1.2232 | Val Loss: 2.3589\n",
      "Epoch: 15 | Loss: 1.1719 | Val Loss: 1.2373\n",
      "Epoch: 16 | Loss: 1.0854 | Val Loss: 1.5518\n",
      "Epoch: 17 | Loss: 1.0367 | Val Loss: 1.1828\n",
      "Epoch: 18 | Loss: 0.9718 | Val Loss: 0.9853\n",
      "Epoch: 19 | Loss: 0.9395 | Val Loss: 1.0430\n",
      "Epoch: 20 | Loss: 0.8954 | Val Loss: 0.9941\n",
      "Epoch: 21 | Loss: 0.8344 | Val Loss: 0.8731\n",
      "Epoch: 22 | Loss: 0.8235 | Val Loss: 0.9557\n",
      "Epoch: 23 | Loss: 0.7990 | Val Loss: 1.1123\n",
      "Epoch: 24 | Loss: 0.7424 | Val Loss: 1.4220\n",
      "Epoch: 25 | Loss: 0.7240 | Val Loss: 0.7789\n",
      "Epoch: 26 | Loss: 0.6827 | Val Loss: 0.8426\n",
      "Epoch: 27 | Loss: 0.6636 | Val Loss: 1.6501\n",
      "Epoch: 28 | Loss: 0.6403 | Val Loss: 0.8218\n",
      "Epoch: 29 | Loss: 0.6306 | Val Loss: 0.9616\n",
      "Epoch: 30 | Loss: 0.6091 | Val Loss: 0.8553\n",
      "Epoch: 31 | Loss: 0.5905 | Val Loss: 0.8974\n",
      "Epoch: 32 | Loss: 0.5776 | Val Loss: 0.7292\n",
      "Epoch: 33 | Loss: 0.5692 | Val Loss: 0.8293\n",
      "Epoch: 34 | Loss: 0.5682 | Val Loss: 1.2058\n",
      "Epoch: 35 | Loss: 0.5520 | Val Loss: 1.2459\n",
      "Epoch: 36 | Loss: 0.5337 | Val Loss: 0.7310\n",
      "Epoch: 37 | Loss: 0.5265 | Val Loss: 0.8119\n",
      "Epoch: 38 | Loss: 0.5196 | Val Loss: 0.6480\n",
      "Epoch: 39 | Loss: 0.5253 | Val Loss: 0.9868\n",
      "Epoch: 40 | Loss: 0.5095 | Val Loss: 0.6347\n",
      "Epoch: 1 | Loss: 4.7045 | Val Loss: 2.6532\n",
      "Epoch: 2 | Loss: 2.4208 | Val Loss: 2.8417\n",
      "Epoch: 3 | Loss: 2.2935 | Val Loss: 1.7772\n",
      "Epoch: 4 | Loss: 2.2158 | Val Loss: 2.2749\n",
      "Epoch: 5 | Loss: 2.1391 | Val Loss: 1.8240\n",
      "Epoch: 6 | Loss: 2.0321 | Val Loss: 1.9355\n",
      "Epoch: 7 | Loss: 1.9637 | Val Loss: 2.2786\n",
      "Epoch: 8 | Loss: 1.8941 | Val Loss: 1.6306\n",
      "Epoch: 9 | Loss: 1.7668 | Val Loss: 1.6223\n",
      "Epoch: 10 | Loss: 1.6854 | Val Loss: 1.7687\n",
      "Epoch: 11 | Loss: 1.6195 | Val Loss: 2.3831\n",
      "Epoch: 12 | Loss: 1.5338 | Val Loss: 1.8601\n",
      "Epoch: 13 | Loss: 1.4638 | Val Loss: 1.3493\n",
      "Epoch: 14 | Loss: 1.4052 | Val Loss: 1.7470\n",
      "Epoch: 15 | Loss: 1.3492 | Val Loss: 1.3934\n",
      "Epoch: 16 | Loss: 1.2773 | Val Loss: 1.5550\n",
      "Epoch: 17 | Loss: 1.2315 | Val Loss: 1.5386\n",
      "Epoch: 18 | Loss: 1.1642 | Val Loss: 1.4601\n",
      "Epoch: 19 | Loss: 1.1367 | Val Loss: 2.0127\n",
      "Epoch: 20 | Loss: 1.0865 | Val Loss: 1.1695\n",
      "Epoch: 21 | Loss: 1.0432 | Val Loss: 1.2192\n",
      "Epoch: 22 | Loss: 1.0150 | Val Loss: 1.3049\n",
      "Epoch: 23 | Loss: 0.9426 | Val Loss: 1.0997\n",
      "Epoch: 24 | Loss: 0.9235 | Val Loss: 1.2116\n",
      "Epoch: 25 | Loss: 0.8657 | Val Loss: 0.9015\n",
      "Epoch: 26 | Loss: 0.8437 | Val Loss: 1.1903\n",
      "Epoch: 27 | Loss: 0.8084 | Val Loss: 1.1193\n",
      "Epoch: 28 | Loss: 0.7849 | Val Loss: 0.9619\n",
      "Epoch: 29 | Loss: 0.7518 | Val Loss: 0.9145\n",
      "Epoch: 30 | Loss: 0.7183 | Val Loss: 0.8888\n",
      "Epoch: 31 | Loss: 0.6995 | Val Loss: 0.9364\n",
      "Epoch: 32 | Loss: 0.6744 | Val Loss: 0.7799\n",
      "Epoch: 33 | Loss: 0.6665 | Val Loss: 0.7905\n",
      "Epoch: 34 | Loss: 0.6432 | Val Loss: 0.7501\n",
      "Epoch: 35 | Loss: 0.6324 | Val Loss: 0.8540\n",
      "Epoch: 36 | Loss: 0.6003 | Val Loss: 0.8320\n",
      "Epoch: 37 | Loss: 0.5888 | Val Loss: 0.8799\n",
      "Epoch: 38 | Loss: 0.5856 | Val Loss: 0.6807\n",
      "Epoch: 39 | Loss: 0.5703 | Val Loss: 0.7444\n",
      "Epoch: 40 | Loss: 0.5640 | Val Loss: 0.6285\n",
      "Epoch: 1 | Loss: 4.9663 | Val Loss: 2.0083\n",
      "Epoch: 2 | Loss: 2.0343 | Val Loss: 1.9694\n",
      "Epoch: 3 | Loss: 1.9301 | Val Loss: 2.0187\n",
      "Epoch: 4 | Loss: 1.8592 | Val Loss: 1.6036\n",
      "Epoch: 5 | Loss: 1.7994 | Val Loss: 1.8259\n",
      "Epoch: 6 | Loss: 1.7685 | Val Loss: 1.9329\n",
      "Epoch: 7 | Loss: 1.6671 | Val Loss: 1.7709\n",
      "Epoch: 8 | Loss: 1.6608 | Val Loss: 1.6092\n",
      "Epoch: 9 | Loss: 1.5552 | Val Loss: 1.5192\n",
      "Epoch: 10 | Loss: 1.5227 | Val Loss: 1.6015\n",
      "Epoch: 11 | Loss: 1.4784 | Val Loss: 1.6894\n",
      "Epoch: 12 | Loss: 1.4043 | Val Loss: 1.5118\n",
      "Epoch: 13 | Loss: 1.3368 | Val Loss: 1.2293\n",
      "Epoch: 14 | Loss: 1.2983 | Val Loss: 1.3149\n",
      "Epoch: 15 | Loss: 1.2413 | Val Loss: 1.1968\n",
      "Epoch: 16 | Loss: 1.1887 | Val Loss: 1.3044\n",
      "Epoch: 17 | Loss: 1.1674 | Val Loss: 1.3668\n",
      "Epoch: 18 | Loss: 1.1433 | Val Loss: 1.1417\n",
      "Epoch: 19 | Loss: 1.0910 | Val Loss: 1.0489\n",
      "Epoch: 20 | Loss: 1.0538 | Val Loss: 1.3801\n",
      "Epoch: 21 | Loss: 1.0162 | Val Loss: 1.1462\n",
      "Epoch: 22 | Loss: 0.9719 | Val Loss: 1.1757\n",
      "Epoch: 23 | Loss: 0.9336 | Val Loss: 0.9254\n",
      "Epoch: 24 | Loss: 0.9088 | Val Loss: 0.8962\n",
      "Epoch: 25 | Loss: 0.8997 | Val Loss: 1.0069\n",
      "Epoch: 26 | Loss: 0.8499 | Val Loss: 1.5067\n",
      "Epoch: 27 | Loss: 0.8287 | Val Loss: 0.8302\n",
      "Epoch: 28 | Loss: 0.8159 | Val Loss: 0.8737\n",
      "Epoch: 29 | Loss: 0.7821 | Val Loss: 1.0035\n",
      "Epoch: 30 | Loss: 0.7421 | Val Loss: 0.9792\n",
      "Epoch: 31 | Loss: 0.7386 | Val Loss: 1.0848\n",
      "Epoch: 32 | Loss: 0.6973 | Val Loss: 0.7144\n",
      "Epoch: 33 | Loss: 0.6784 | Val Loss: 0.8117\n",
      "Epoch: 34 | Loss: 0.6519 | Val Loss: 0.6782\n",
      "Epoch: 35 | Loss: 0.6682 | Val Loss: 0.8195\n",
      "Epoch: 36 | Loss: 0.6283 | Val Loss: 0.6745\n",
      "Epoch: 37 | Loss: 0.6308 | Val Loss: 0.7477\n",
      "Epoch: 38 | Loss: 0.6041 | Val Loss: 0.8292\n",
      "Epoch: 39 | Loss: 0.5760 | Val Loss: 0.6940\n",
      "Epoch: 40 | Loss: 0.5860 | Val Loss: 0.8345\n",
      "Epoch: 1 | Loss: 5.1775 | Val Loss: 1.4347\n",
      "Epoch: 2 | Loss: 1.5350 | Val Loss: 1.4832\n",
      "Epoch: 3 | Loss: 1.4632 | Val Loss: 1.4984\n",
      "Epoch: 4 | Loss: 1.3988 | Val Loss: 1.1613\n",
      "Epoch: 5 | Loss: 1.3603 | Val Loss: 1.7154\n",
      "Epoch: 6 | Loss: 1.3276 | Val Loss: 1.0967\n",
      "Epoch: 7 | Loss: 1.3131 | Val Loss: 1.3714\n",
      "Epoch: 8 | Loss: 1.2737 | Val Loss: 1.3199\n",
      "Epoch: 9 | Loss: 1.2317 | Val Loss: 1.4410\n",
      "Epoch: 10 | Loss: 1.2452 | Val Loss: 1.3796\n",
      "Epoch: 11 | Loss: 1.2012 | Val Loss: 1.3088\n",
      "Epoch: 12 | Loss: 1.1878 | Val Loss: 1.1629\n",
      "Epoch: 13 | Loss: 1.1683 | Val Loss: 1.1073\n",
      "Epoch: 14 | Loss: 1.1517 | Val Loss: 1.2822\n",
      "Epoch: 15 | Loss: 1.1099 | Val Loss: 1.0636\n",
      "Epoch: 16 | Loss: 1.1132 | Val Loss: 1.0178\n",
      "Epoch: 17 | Loss: 1.0582 | Val Loss: 1.1705\n",
      "Epoch: 18 | Loss: 1.0434 | Val Loss: 1.1100\n",
      "Epoch: 19 | Loss: 1.0127 | Val Loss: 1.1275\n",
      "Epoch: 20 | Loss: 0.9890 | Val Loss: 1.1174\n",
      "Epoch: 21 | Loss: 0.9687 | Val Loss: 1.0402\n",
      "Epoch: 22 | Loss: 0.9461 | Val Loss: 1.0067\n",
      "Epoch: 23 | Loss: 0.9263 | Val Loss: 0.9294\n",
      "Epoch: 24 | Loss: 0.9043 | Val Loss: 1.1075\n",
      "Epoch: 25 | Loss: 0.8879 | Val Loss: 1.0473\n",
      "Epoch: 26 | Loss: 0.8723 | Val Loss: 1.0012\n",
      "Epoch: 27 | Loss: 0.8497 | Val Loss: 1.3938\n",
      "Epoch: 28 | Loss: 0.8191 | Val Loss: 0.8623\n",
      "Epoch: 29 | Loss: 0.7951 | Val Loss: 1.0253\n",
      "Epoch: 30 | Loss: 0.7935 | Val Loss: 1.1356\n",
      "Epoch: 31 | Loss: 0.7698 | Val Loss: 0.9080\n",
      "Epoch: 32 | Loss: 0.7603 | Val Loss: 1.2687\n",
      "Epoch: 33 | Loss: 0.7505 | Val Loss: 0.8087\n",
      "Epoch: 34 | Loss: 0.7264 | Val Loss: 0.8570\n",
      "Epoch: 35 | Loss: 0.7135 | Val Loss: 1.0373\n",
      "Epoch: 36 | Loss: 0.7081 | Val Loss: 0.7684\n",
      "Epoch: 37 | Loss: 0.6713 | Val Loss: 0.6903\n",
      "Epoch: 38 | Loss: 0.6610 | Val Loss: 0.7111\n",
      "Epoch: 39 | Loss: 0.6750 | Val Loss: 0.8403\n",
      "Epoch: 40 | Loss: 0.6400 | Val Loss: 0.8037\n",
      "Epoch: 1 | Loss: 5.2754 | Val Loss: 0.9881\n",
      "Epoch: 2 | Loss: 0.8840 | Val Loss: 0.9484\n",
      "Epoch: 3 | Loss: 0.8358 | Val Loss: 0.7944\n",
      "Epoch: 4 | Loss: 0.7980 | Val Loss: 0.8001\n",
      "Epoch: 5 | Loss: 0.7882 | Val Loss: 0.8281\n",
      "Epoch: 6 | Loss: 0.7520 | Val Loss: 0.7813\n",
      "Epoch: 7 | Loss: 0.7430 | Val Loss: 0.7490\n",
      "Epoch: 8 | Loss: 0.7407 | Val Loss: 0.7520\n",
      "Epoch: 9 | Loss: 0.7394 | Val Loss: 0.7391\n",
      "Epoch: 10 | Loss: 0.7580 | Val Loss: 0.8143\n",
      "Epoch: 11 | Loss: 0.7196 | Val Loss: 0.6909\n",
      "Epoch: 12 | Loss: 0.7045 | Val Loss: 0.6942\n",
      "Epoch: 13 | Loss: 0.7060 | Val Loss: 0.9583\n",
      "Epoch: 14 | Loss: 0.7020 | Val Loss: 0.9318\n",
      "Epoch: 15 | Loss: 0.6912 | Val Loss: 0.6802\n",
      "Epoch: 16 | Loss: 0.6835 | Val Loss: 0.7367\n",
      "Epoch: 17 | Loss: 0.6828 | Val Loss: 0.6860\n",
      "Epoch: 18 | Loss: 0.6663 | Val Loss: 0.7885\n",
      "Epoch: 19 | Loss: 0.6753 | Val Loss: 0.7144\n",
      "Epoch: 20 | Loss: 0.6471 | Val Loss: 0.7738\n",
      "Epoch: 21 | Loss: 0.6558 | Val Loss: 0.6393\n",
      "Epoch: 22 | Loss: 0.6393 | Val Loss: 0.7740\n",
      "Epoch: 23 | Loss: 0.6255 | Val Loss: 0.5465\n",
      "Epoch: 24 | Loss: 0.6344 | Val Loss: 0.7601\n",
      "Epoch: 25 | Loss: 0.6281 | Val Loss: 0.6288\n",
      "Epoch: 26 | Loss: 0.6073 | Val Loss: 0.5039\n",
      "Epoch: 27 | Loss: 0.6122 | Val Loss: 0.6814\n",
      "Epoch: 28 | Loss: 0.6028 | Val Loss: 0.5749\n",
      "Epoch: 29 | Loss: 0.6007 | Val Loss: 0.6851\n",
      "Epoch: 30 | Loss: 0.5927 | Val Loss: 0.7954\n",
      "Epoch: 31 | Loss: 0.5882 | Val Loss: 0.5638\n",
      "Epoch: 32 | Loss: 0.5689 | Val Loss: 0.7394\n",
      "Epoch: 33 | Loss: 0.5640 | Val Loss: 0.7094\n",
      "Epoch: 34 | Loss: 0.5586 | Val Loss: 0.5504\n",
      "Epoch: 35 | Loss: 0.5475 | Val Loss: 0.6813\n",
      "Epoch: 36 | Loss: 0.5315 | Val Loss: 0.6834\n",
      "Epoch: 37 | Loss: 0.5277 | Val Loss: 1.0016\n",
      "Epoch: 38 | Loss: 0.5361 | Val Loss: 0.4779\n",
      "Epoch: 39 | Loss: 0.5230 | Val Loss: 0.5001\n",
      "Epoch: 40 | Loss: 0.5038 | Val Loss: 0.7220\n"
     ]
    }
   ],
   "source": [
    "qlist = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "epochs = 40\n",
    "\n",
    "all_models = []\n",
    "for q in qlist:\n",
    "    temp_model = trainCNN(epochs, all_blocks, q)\n",
    "    all_models.append(temp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, 10.3519, 18.5233, 25.7514, 35.1429,\n",
       "         45.6047, 53.3597, 62.5164, 67.6771, 73.8580, 64.1007, 64.0721, 69.7000,\n",
       "         56.7913, 39.7921, 42.3076, 42.9325, 43.5416, 36.1186, 24.8013, 21.5130,\n",
       "         29.4884, 22.9956, 11.6964,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  5.8725, 10.9380, 16.3496, 24.0004, 28.6765,\n",
       "         41.3924, 49.4036, 54.1940, 64.8875, 70.2591, 65.3948, 51.0579, 42.9892,\n",
       "         34.4777, 30.9703, 18.5401, 31.7124, 37.8042, 28.2661, 25.8330, 14.9887,\n",
       "         12.1981, 12.8575, 21.4388,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<MaximumBackward>)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fit_3.eval()\n",
    "model_fit_3(X_test_scaled.unsqueeze(0).permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  2.7911, 10.2532, 21.8295, 29.3181, 39.5143, 49.3631,\n",
       "         59.0885, 67.4031, 75.5168, 82.6882, 88.1655, 91.8142, 94.7293, 98.3151,\n",
       "         99.6122, 98.5456, 96.4506, 94.9549, 90.8239, 84.3968, 77.3709, 69.1803,\n",
       "         60.0362, 51.8653, 41.1143, 31.0865, 20.9020, 12.0363,  4.1673,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, 11.7293, 21.1347, 30.1212, 39.4605, 49.0026,\n",
       "         58.0772, 66.6953, 74.3474, 80.5856, 86.0345, 88.9672, 90.9638, 93.7306,\n",
       "         93.9360, 92.3277, 92.0909, 87.9208, 83.1364, 78.2522, 71.6307, 63.4750,\n",
       "         54.8168, 47.5863, 38.9608, 29.7340, 20.2401, 12.1814,  5.1140,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<MaximumBackward>)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fit_2.eval()\n",
    "model_fit_2(X_test_scaled.unsqueeze(0).permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  6.7767, 13.6113, 20.9538, 28.2056, 37.3899,\n",
       "         49.4168, 57.1391, 64.8364, 69.0400, 64.1241, 56.3657, 40.4668, 42.2490,\n",
       "         57.6851, 78.4175, 70.3783, 61.8727, 56.6361, 55.5211, 51.8121, 40.8567,\n",
       "         41.8397, 36.8074, 30.5164, 23.1708, 13.7557,  6.4310,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  7.4615, 14.6673, 23.4963, 31.9420, 40.0800,\n",
       "         51.5010, 61.4924, 70.6849, 82.1008, 82.9252, 86.2703, 84.5968, 87.4733,\n",
       "         76.5311, 76.9198, 74.5990, 60.1821, 62.1870, 56.3080, 48.8988, 31.9921,\n",
       "         28.9631, 23.8986, 19.0518, 16.7852, 10.0963,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<MaximumBackward>)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fit_2.eval()\n",
    "model_fit_2(X_test_scaled.unsqueeze(0).permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintp, trainytp, _, _ = pick_one_batch(1, 1, all_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0213, 0.0426,  ..., 0.9574, 0.9787, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [1.0780, 1.1873, 1.1675,  ..., 0.9916, 1.0127, 1.0120],\n",
       "         [1.8000, 1.8000, 1.8000,  ..., 1.8667, 1.8667, 1.8000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 14.8106, 24.0457, 32.9695,\n",
       "         37.2642, 44.1790, 53.7278, 66.0208, 73.4586, 48.7767, 62.5999, 55.5483,\n",
       "         37.7692, 30.8488, 36.1534, 57.2513, 59.9888, 39.2756, 22.5861, 22.9690,\n",
       "         17.2192, 21.8272, 16.3000, 11.9697,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  7.7782, 11.2605, 18.2231, 25.9930,\n",
       "         35.5062, 42.3293, 50.7710, 73.3819, 79.6653, 82.6055, 58.2425, 51.9659,\n",
       "         41.1143, 39.7554, 17.5615, 37.9111, 34.1295, 55.9844, 40.5281, 18.4217,\n",
       "         25.1828, 17.8163, 17.2617, 13.4249,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<MaximumBackward>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = all_models[0]\n",
    "aa(X_test_scaled.unsqueeze(0).permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(base_dir, 'test')\n",
    "lists = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = []\n",
    "result_pd = pd.DataFrame()\n",
    "\n",
    "for i in range(81):\n",
    "    file_path = os.path.join(test_dir, str(i)+'.csv')\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp['Time'] = temp['Hour']*60 + temp['Minute']\n",
    "    temp = temp.loc[(temp.Day >= 4) & (temp.Day <= 6), :][['Time', 'DHI', 'DNI', 'WS', 'RH', 'T', 'TARGET']]\n",
    "    \n",
    "    X_test_scaled = torch.tensor(scaler.transform(temp.values)).float()\n",
    "    \n",
    "    temp_result_list = []\n",
    "    for m in range(9):\n",
    "        temp_model = all_models[m]\n",
    "        temp_model.eval()\n",
    "        temp_result = temp_model(X_test_scaled.unsqueeze(0).permute(0,2,1)).detach().to('cpu').numpy().squeeze(0)\n",
    "        temp_result_list.append(pd.DataFrame(temp_result))\n",
    "    \n",
    "    result_pd = result_pd.append(pd.concat(temp_result_list, axis=1))\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7776 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    0    0    0    0    0    0    0    0\n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "91  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "92  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "93  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "94  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "95  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[7776 rows x 9 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.090574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.262747</td>\n",
       "      <td>5.125685</td>\n",
       "      <td>6.982367</td>\n",
       "      <td>4.937912</td>\n",
       "      <td>3.339508</td>\n",
       "      <td>5.024149</td>\n",
       "      <td>5.162632</td>\n",
       "      <td>7.457860</td>\n",
       "      <td>7.829621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.976308</td>\n",
       "      <td>9.470694</td>\n",
       "      <td>13.429438</td>\n",
       "      <td>12.084684</td>\n",
       "      <td>8.294762</td>\n",
       "      <td>12.951414</td>\n",
       "      <td>12.279289</td>\n",
       "      <td>15.107727</td>\n",
       "      <td>14.396976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24.761511</td>\n",
       "      <td>14.491241</td>\n",
       "      <td>18.033062</td>\n",
       "      <td>19.191772</td>\n",
       "      <td>9.747635</td>\n",
       "      <td>20.427896</td>\n",
       "      <td>21.239132</td>\n",
       "      <td>22.900375</td>\n",
       "      <td>20.940405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>34.736671</td>\n",
       "      <td>23.463503</td>\n",
       "      <td>23.289780</td>\n",
       "      <td>24.858582</td>\n",
       "      <td>19.146358</td>\n",
       "      <td>30.012133</td>\n",
       "      <td>32.248135</td>\n",
       "      <td>33.011116</td>\n",
       "      <td>27.540174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40.038322</td>\n",
       "      <td>26.615726</td>\n",
       "      <td>30.138552</td>\n",
       "      <td>31.151026</td>\n",
       "      <td>30.191957</td>\n",
       "      <td>36.339600</td>\n",
       "      <td>38.769321</td>\n",
       "      <td>38.016155</td>\n",
       "      <td>34.168808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>39.218670</td>\n",
       "      <td>31.447020</td>\n",
       "      <td>32.679646</td>\n",
       "      <td>32.827881</td>\n",
       "      <td>30.048162</td>\n",
       "      <td>43.521439</td>\n",
       "      <td>42.049824</td>\n",
       "      <td>42.651669</td>\n",
       "      <td>39.110832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40.874485</td>\n",
       "      <td>38.180111</td>\n",
       "      <td>38.787949</td>\n",
       "      <td>30.161415</td>\n",
       "      <td>33.899483</td>\n",
       "      <td>47.250607</td>\n",
       "      <td>49.172054</td>\n",
       "      <td>48.079853</td>\n",
       "      <td>42.892654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>48.972469</td>\n",
       "      <td>39.887230</td>\n",
       "      <td>38.996445</td>\n",
       "      <td>41.063999</td>\n",
       "      <td>42.933353</td>\n",
       "      <td>49.298149</td>\n",
       "      <td>58.684315</td>\n",
       "      <td>52.035290</td>\n",
       "      <td>45.953510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>47.395885</td>\n",
       "      <td>39.564533</td>\n",
       "      <td>32.844868</td>\n",
       "      <td>40.840252</td>\n",
       "      <td>36.226490</td>\n",
       "      <td>44.847626</td>\n",
       "      <td>57.756451</td>\n",
       "      <td>53.092117</td>\n",
       "      <td>47.887165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>39.067341</td>\n",
       "      <td>44.953426</td>\n",
       "      <td>33.246193</td>\n",
       "      <td>42.067337</td>\n",
       "      <td>41.970520</td>\n",
       "      <td>54.462814</td>\n",
       "      <td>54.800529</td>\n",
       "      <td>51.276913</td>\n",
       "      <td>46.752037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>41.231022</td>\n",
       "      <td>44.585377</td>\n",
       "      <td>30.199568</td>\n",
       "      <td>34.144642</td>\n",
       "      <td>39.967533</td>\n",
       "      <td>45.711010</td>\n",
       "      <td>54.355618</td>\n",
       "      <td>48.311710</td>\n",
       "      <td>46.001701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33.792828</td>\n",
       "      <td>42.854019</td>\n",
       "      <td>26.477499</td>\n",
       "      <td>33.441570</td>\n",
       "      <td>30.208847</td>\n",
       "      <td>43.438770</td>\n",
       "      <td>48.970924</td>\n",
       "      <td>45.048225</td>\n",
       "      <td>42.117847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32.158440</td>\n",
       "      <td>41.270847</td>\n",
       "      <td>30.434938</td>\n",
       "      <td>33.663616</td>\n",
       "      <td>34.598270</td>\n",
       "      <td>39.426289</td>\n",
       "      <td>45.776325</td>\n",
       "      <td>42.080730</td>\n",
       "      <td>38.072765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>27.632395</td>\n",
       "      <td>38.023232</td>\n",
       "      <td>16.495392</td>\n",
       "      <td>22.113066</td>\n",
       "      <td>28.681650</td>\n",
       "      <td>26.724373</td>\n",
       "      <td>36.881924</td>\n",
       "      <td>34.394875</td>\n",
       "      <td>31.392281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>15.817272</td>\n",
       "      <td>25.281485</td>\n",
       "      <td>15.557605</td>\n",
       "      <td>19.979376</td>\n",
       "      <td>19.784174</td>\n",
       "      <td>19.098730</td>\n",
       "      <td>27.076260</td>\n",
       "      <td>26.480648</td>\n",
       "      <td>25.033840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.943742</td>\n",
       "      <td>17.905176</td>\n",
       "      <td>9.529296</td>\n",
       "      <td>12.680797</td>\n",
       "      <td>11.738889</td>\n",
       "      <td>8.885252</td>\n",
       "      <td>21.631598</td>\n",
       "      <td>17.925045</td>\n",
       "      <td>17.475485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.092191</td>\n",
       "      <td>7.554016</td>\n",
       "      <td>7.130516</td>\n",
       "      <td>6.930404</td>\n",
       "      <td>9.534465</td>\n",
       "      <td>5.621132</td>\n",
       "      <td>11.405036</td>\n",
       "      <td>10.061366</td>\n",
       "      <td>10.100936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287412</td>\n",
       "      <td>0.805652</td>\n",
       "      <td>0.448213</td>\n",
       "      <td>1.265576</td>\n",
       "      <td>0.991511</td>\n",
       "      <td>3.831720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "16   5.262747   5.125685   6.982367   4.937912   3.339508   5.024149   \n",
       "17  14.976308   9.470694  13.429438  12.084684   8.294762  12.951414   \n",
       "18  24.761511  14.491241  18.033062  19.191772   9.747635  20.427896   \n",
       "19  34.736671  23.463503  23.289780  24.858582  19.146358  30.012133   \n",
       "20  40.038322  26.615726  30.138552  31.151026  30.191957  36.339600   \n",
       "21  39.218670  31.447020  32.679646  32.827881  30.048162  43.521439   \n",
       "22  40.874485  38.180111  38.787949  30.161415  33.899483  47.250607   \n",
       "23  48.972469  39.887230  38.996445  41.063999  42.933353  49.298149   \n",
       "24  47.395885  39.564533  32.844868  40.840252  36.226490  44.847626   \n",
       "25  39.067341  44.953426  33.246193  42.067337  41.970520  54.462814   \n",
       "26  41.231022  44.585377  30.199568  34.144642  39.967533  45.711010   \n",
       "27  33.792828  42.854019  26.477499  33.441570  30.208847  43.438770   \n",
       "28  32.158440  41.270847  30.434938  33.663616  34.598270  39.426289   \n",
       "29  27.632395  38.023232  16.495392  22.113066  28.681650  26.724373   \n",
       "30  15.817272  25.281485  15.557605  19.979376  19.784174  19.098730   \n",
       "31   7.943742  17.905176   9.529296  12.680797  11.738889   8.885252   \n",
       "32   1.092191   7.554016   7.130516   6.930404   9.534465   5.621132   \n",
       "33   0.000000   0.000000   0.000000   1.287412   0.805652   0.448213   \n",
       "34   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000  \n",
       "15   0.000000   0.000000   1.090574  \n",
       "16   5.162632   7.457860   7.829621  \n",
       "17  12.279289  15.107727  14.396976  \n",
       "18  21.239132  22.900375  20.940405  \n",
       "19  32.248135  33.011116  27.540174  \n",
       "20  38.769321  38.016155  34.168808  \n",
       "21  42.049824  42.651669  39.110832  \n",
       "22  49.172054  48.079853  42.892654  \n",
       "23  58.684315  52.035290  45.953510  \n",
       "24  57.756451  53.092117  47.887165  \n",
       "25  54.800529  51.276913  46.752037  \n",
       "26  54.355618  48.311710  46.001701  \n",
       "27  48.970924  45.048225  42.117847  \n",
       "28  45.776325  42.080730  38.072765  \n",
       "29  36.881924  34.394875  31.392281  \n",
       "30  27.076260  26.480648  25.033840  \n",
       "31  21.631598  17.925045  17.475485  \n",
       "32  11.405036  10.061366  10.100936  \n",
       "33   1.265576   0.991511   3.831720  \n",
       "34   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pd.iloc[:48, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 7)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.concat(df_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = torch.tensor(scaler.transform(X_test.values)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3822,\n",
       "          4.9136, 12.8338, 15.7965, 21.9989, 24.5731, 25.1446, 33.5316, 39.8747,\n",
       "         41.4034, 45.7704, 34.9744, 30.8550, 30.5020, 25.3337, 27.3108, 14.1588,\n",
       "          7.3356,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2800,\n",
       "          6.5677, 14.9884, 20.0625, 23.7999, 25.7666, 34.4319, 49.9423, 47.7675,\n",
       "         41.2324, 44.8759, 37.3615, 35.7089, 41.2612, 34.9380, 26.2835, 19.5008,\n",
       "         10.4811,  0.1983,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<MaximumBackward>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fit_1.eval()\n",
    "model_fit_1(X_test_scaled.unsqueeze(0).permute(0,2,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "dacon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
