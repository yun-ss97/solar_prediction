{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn import ensemble\n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import scipy as sp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "train_dir = os.path.join(base_dir, 'train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.08</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.06</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.78</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.75</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>75.20</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Hour  Minute  DHI  DNI   WS     RH   T  TARGET\n",
       "0    0     0       0    0    0  1.5  69.08 -12     0.0\n",
       "1    0     0      30    0    0  1.5  69.06 -12     0.0\n",
       "2    0     1       0    0    0  1.6  71.78 -12     0.0\n",
       "3    0     1      30    0    0  1.6  71.75 -12     0.0\n",
       "4    0     2       0    0    0  1.6  75.20 -12     0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df = pd.read_csv(train_dir)\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df['Time'] = base_df['Hour']*60 + base_df['Minute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift\n",
    "shift_df = base_df.copy()\n",
    "\n",
    "shift_df['TARGET1'] = shift_df['TARGET'].shift(-48).fillna(method='ffill')\n",
    "shift_df['TARGET2'] = shift_df['TARGET'].shift(-96).fillna(method='ffill')\n",
    "\n",
    "shift_df_result = shift_df.iloc[:-96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>Time</th>\n",
       "      <th>TARGET1</th>\n",
       "      <th>TARGET2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.08</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.06</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.78</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Hour  Minute  DHI  DNI   WS     RH   T  TARGET  Time  TARGET1  TARGET2\n",
       "0    0     0       0    0    0  1.5  69.08 -12     0.0     0      0.0      0.0\n",
       "1    0     0      30    0    0  1.5  69.06 -12     0.0    30      0.0      0.0\n",
       "2    0     1       0    0    0  1.6  71.78 -12     0.0    60      0.0      0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_df_result.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-9bbaf2187cd0>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  shift_df_result['DP'] = dp\n"
     ]
    }
   ],
   "source": [
    "# Td, T-Td\n",
    "b = 17.62\n",
    "c = 243.12\n",
    "term1 = b*shift_df_result['T']/(c + shift_df_result['T'])\n",
    "term2 = np.log(shift_df_result['RH']/100)\n",
    "gamma = term1 + term2\n",
    "dp = (c*gamma)/(b-gamma)\n",
    "\n",
    "shift_df_result['DP'] = dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>Time</th>\n",
       "      <th>TARGET1</th>\n",
       "      <th>TARGET2</th>\n",
       "      <th>DP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.08</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.522271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>69.06</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.525742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.78</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.061776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Hour  Minute  DHI  DNI   WS     RH   T  TARGET  Time  TARGET1  \\\n",
       "0    0     0       0    0    0  1.5  69.08 -12     0.0     0      0.0   \n",
       "1    0     0      30    0    0  1.5  69.06 -12     0.0    30      0.0   \n",
       "2    0     1       0    0    0  1.6  71.78 -12     0.0    60      0.0   \n",
       "\n",
       "   TARGET2         DP  \n",
       "0      0.0 -16.522271  \n",
       "1      0.0 -16.525742  \n",
       "2      0.0 -16.061776  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_df_result.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift_df_result = shift_df_result.loc[(shift_df_result.Hour >= 3) & (shift_df_result.Hour <= 22), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "# (1) Get rid of time variables\n",
    "df_vars = shift_df_result[['Time', 'DHI', 'DNI', 'WS', 'RH', 'T', 'DP', 'TARGET']]\n",
    "df_label = shift_df_result[['TARGET1', 'TARGET2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_torch_dataset(scale=True, test_size = 0.2, *datasets):\n",
    "    \n",
    "    # To torch\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    train_x, val_x, train_y, val_y = tts(df_vars, df_label, test_size = test_size, random_state = 2020)\n",
    "    if scale:\n",
    "        scaled = StandardScaler()\n",
    "        scaled.fit(train_x)\n",
    "        \n",
    "        train_x = scaled.transform(train_x)\n",
    "        val_x = scaled.transform(val_x)\n",
    "        \n",
    "        train_x_torch = torch.tensor(train_x).float().to(device)\n",
    "        train_y_torch = torch.tensor(train_y.values).float().to(device)\n",
    "\n",
    "        val_x_torch = torch.tensor(val_x).float().to(device)\n",
    "        val_y_torch = torch.tensor(val_y.values).float().to(device)\n",
    "        \n",
    "    else:\n",
    "        train_x_torch = torch.tensor(train_x.values).float().to(device)\n",
    "        train_y_torch = torch.tensor(train_y.values).float().to(device)\n",
    "\n",
    "        val_x_torch = torch.tensor(val_x.values).float().to(device)\n",
    "        val_y_torch = torch.tensor(val_y.values).float().to(device)\n",
    "    \n",
    "    \n",
    "    return train_x_torch, val_x_torch, train_y_torch, val_y_torch, scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_torch, val_x_torch, train_y_torch, val_y_torch, scaled = make_torch_dataset(True, 0.25, df_vars, df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### batch iterator\n",
    "batch_size = 128\n",
    "\n",
    "trainset = data_utils.TensorDataset(train_x_torch, train_y_torch)\n",
    "train_loader = data_utils.DataLoader(trainset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "valset = data_utils.TensorDataset(val_x_torch, val_y_torch)\n",
    "val_loader = data_utils.DataLoader(valset, batch_size = len(valset), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "class DaconNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, x_features, dropout_prob):\n",
    "        super(DaconNN, self).__init__()\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.block1_in = 512\n",
    "        self.block1_hid = 1024\n",
    "        \n",
    "        # block1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(x_features, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(self.block1_in, self.block1_hid),\n",
    "            nn.BatchNorm1d(self.block1_hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(self.block1_hid, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "        )\n",
    "        \n",
    "        self.block1_rep = nn.Sequential(\n",
    "            nn.Linear(self.block1_in, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(self.block1_in, self.block1_hid),\n",
    "            nn.BatchNorm1d(self.block1_hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(self.block1_hid, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "        )\n",
    "        \n",
    "        self.block1_shortcut = nn.Sequential(\n",
    "            nn.Linear(x_features, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "        )\n",
    "        \n",
    "        # block2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(self.block1_in, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(64, 9),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        res = self.block1_shortcut(x)\n",
    "        \n",
    "        # forward block1\n",
    "        out = self.block1(x)\n",
    "        #out = out + res\n",
    "\n",
    "        # forward block2\n",
    "        out = self.block2(out)\n",
    "        out = torch.max(torch.zeros_like(out), out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "class DaconNN_2(nn.Module):\n",
    " \n",
    "    def __init__(self, x_features, dropout_prob):\n",
    "        super(DaconNN_2, self).__init__()\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.block1_in = 512\n",
    "        self.block1_hid = 1024\n",
    "        \n",
    "        # block1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(x_features, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(self.block1_in, self.block1_hid),\n",
    "            nn.BatchNorm1d(self.block1_hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(self.block1_hid, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "        )\n",
    "        \n",
    "        self.block1_rep = nn.Sequential(\n",
    "            nn.Linear(self.block1_in, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(self.block1_in, self.block1_hid),\n",
    "            nn.BatchNorm1d(self.block1_hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(self.block1_hid, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "        )\n",
    "        \n",
    "        self.block1_shortcut = nn.Sequential(\n",
    "            nn.Linear(x_features, self.block1_in),\n",
    "            nn.BatchNorm1d(self.block1_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "        )\n",
    "        \n",
    "        # block2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(self.block1_in, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = self.dropout_prob),\n",
    "            \n",
    "            nn.Linear(64, 9),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        res = self.block1_shortcut(x)\n",
    "        \n",
    "        # forward block1\n",
    "        out = self.block1(x)\n",
    "        #out = out + res\n",
    "\n",
    "        # forward block2\n",
    "        out = self.block2(out)\n",
    "        out = torch.max(torch.zeros_like(out), out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using gpu (if available)\n",
    "# model = DaconNN(x_features = train_x_torch.shape[1], dropout_prob = 0.1)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device) # cuda:0\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Define Loss function (Quantile Loss)\n",
    "# def QuantileLoss(pred, gt, *qs):\n",
    "#     q1, q2, q3 = qs\n",
    "    \n",
    "#     e1 = pred[:, 0:1] - gt\n",
    "#     e2 = pred[:, 1:2] - gt\n",
    "#     e3 = pred[:, 2:3] - gt\n",
    "    \n",
    "#     eq1 = torch.max(q1*e1, (q1-1)*e1)\n",
    "#     eq2 = torch.max(q2*e2, (q2-1)*e2)\n",
    "#     eq3 = torch.max(q3*e3, (q3-1)*e3)\n",
    "    \n",
    "#     loss = torch.mean(torch.sum(eq1, eq2, eq3))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss function (Quantile Loss)\n",
    "# def QuantileLoss(q, pred, gt):\n",
    "#     loss = torch.mean(torch.max((gt-pred)*q, (pred-gt)*(1-q)))\n",
    "#     return loss\n",
    "\n",
    "# def QuantileLoss(q, pred, gt):\n",
    "#     loss = (gt-pred)*q if gt >= pred else (pred-gt)*(1-q)\n",
    "#     #loss = torch.mean(torch.max((gt-pred)*q, (pred-gt)*(1-q)))\n",
    "#     return loss\n",
    "\n",
    "def QuantileLoss(qs, pred, gt):\n",
    "    #qs = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    qs = qs if isinstance(qs, list) else [qs]\n",
    "    \n",
    "    sum_loss = 0\n",
    "    for i, q in enumerate(qs):\n",
    "        loss = pred[:, i] - gt\n",
    "        loss = torch.max(q*loss, (q-1)*loss)\n",
    "        sum_loss += torch.mean(loss)\n",
    "    \n",
    "#     symloss1 = torch.mean(torch.abs(torch.abs((pred[:, 4] - pred[:, 0]) - torch.abs(pred[:, 4] - pred[:, 8]))))\n",
    "#     symloss2 = torch.mean(torch.abs(torch.abs((pred[:, 4] - pred[:, 1]) - torch.abs(pred[:, 4] - pred[:, 7]))))\n",
    "#     symloss3 = torch.mean(torch.abs(torch.abs((pred[:, 4] - pred[:, 2]) - torch.abs(pred[:, 4] - pred[:, 6]))))\n",
    "#     symloss4 = torch.mean(torch.abs(torch.abs((pred[:, 4] - pred[:, 3]) - torch.abs(pred[:, 4] - pred[:, 5]))))\n",
    "    \n",
    "#     symloss = symloss1 + symloss2 + symloss3 + symloss4\n",
    "    \n",
    "    fin_loss = sum_loss/len(qs)# + 0.05*symloss/4\n",
    "    return fin_loss\n",
    "\n",
    "def train_dnn(epochs, target, qs, dropout, model_type, *loaders):\n",
    "    \n",
    "    train_loader = loaders[0]\n",
    "    val_loader = loaders[1]\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if model_type == '2':\n",
    "        model = DaconNN_2(x_features = train_x_torch.shape[1], dropout_prob = dropout)\n",
    "    else:\n",
    "        model = DaconNN(x_features = train_x_torch.shape[1], dropout_prob = dropout)\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    learning_rate = 0.005\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    decayRate = 0.998\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = optimizer, gamma = decayRate)\n",
    "\n",
    "    train_loss_sum = 0.0\n",
    "    val_loss_sum = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for i, train_block in enumerate(train_loader):\n",
    "\n",
    "            train_X, train_Y = train_block[0], train_block[1]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # loss 계산\n",
    "            train_pred = model(train_X)\n",
    "            train_loss = QuantileLoss(qs, train_pred, train_Y[:, target-1])\n",
    "            train_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_sum += train_loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_set = iter(val_loader).next()\n",
    "            val_X, val_Y = val_set[0], val_set[1]\n",
    "            \n",
    "            val_pred = model(val_X)\n",
    "            val_loss = QuantileLoss(qs, val_pred, val_Y[:, target-1])\n",
    "            val_loss = torch.sum(val_loss)\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1} | Loss: {train_loss_sum.item()/len(train_loader):.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "        \n",
    "        train_loss_sum = 0.0\n",
    "        \n",
    "        # learning rate decaying\n",
    "        lr_scheduler.step()\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qs = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "param_dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 3.3631 | Val Loss: 2.3147\n",
      "Epoch: 2 | Loss: 2.4455 | Val Loss: 2.3238\n",
      "Epoch: 3 | Loss: 2.3944 | Val Loss: 2.2658\n",
      "Epoch: 4 | Loss: 2.3645 | Val Loss: 2.2640\n",
      "Epoch: 5 | Loss: 2.3651 | Val Loss: 2.2408\n",
      "Epoch: 6 | Loss: 2.3193 | Val Loss: 2.2881\n",
      "Epoch: 7 | Loss: 2.3086 | Val Loss: 2.2241\n",
      "Epoch: 8 | Loss: 2.2947 | Val Loss: 2.2374\n",
      "Epoch: 9 | Loss: 2.2948 | Val Loss: 2.2530\n",
      "Epoch: 10 | Loss: 2.2724 | Val Loss: 2.2158\n",
      "Epoch: 11 | Loss: 2.2774 | Val Loss: 2.3739\n",
      "Epoch: 12 | Loss: 2.2644 | Val Loss: 2.2223\n",
      "Epoch: 13 | Loss: 2.2727 | Val Loss: 2.3019\n",
      "Epoch: 14 | Loss: 2.2657 | Val Loss: 2.2593\n",
      "Epoch: 15 | Loss: 2.2446 | Val Loss: 2.2092\n",
      "Epoch: 16 | Loss: 2.2505 | Val Loss: 2.1976\n",
      "Epoch: 17 | Loss: 2.2425 | Val Loss: 2.2184\n",
      "Epoch: 18 | Loss: 2.2299 | Val Loss: 2.2037\n",
      "Epoch: 19 | Loss: 2.2293 | Val Loss: 2.1989\n",
      "Epoch: 20 | Loss: 2.2283 | Val Loss: 2.2032\n",
      "Epoch: 21 | Loss: 2.2338 | Val Loss: 2.1943\n",
      "Epoch: 22 | Loss: 2.2177 | Val Loss: 2.1797\n",
      "Epoch: 23 | Loss: 2.2177 | Val Loss: 2.1530\n",
      "Epoch: 24 | Loss: 2.2209 | Val Loss: 2.1907\n",
      "Epoch: 25 | Loss: 2.2119 | Val Loss: 2.2421\n",
      "Epoch: 26 | Loss: 2.2050 | Val Loss: 2.2025\n",
      "Epoch: 27 | Loss: 2.2187 | Val Loss: 2.1656\n",
      "Epoch: 28 | Loss: 2.2066 | Val Loss: 2.1834\n",
      "Epoch: 29 | Loss: 2.1970 | Val Loss: 2.1724\n",
      "Epoch: 30 | Loss: 2.1994 | Val Loss: 2.1695\n",
      "Epoch: 31 | Loss: 2.1963 | Val Loss: 2.1942\n",
      "Epoch: 32 | Loss: 2.1951 | Val Loss: 2.1654\n",
      "Epoch: 33 | Loss: 2.1885 | Val Loss: 2.1712\n",
      "Epoch: 34 | Loss: 2.1905 | Val Loss: 2.1607\n",
      "Epoch: 35 | Loss: 2.1829 | Val Loss: 2.1575\n",
      "Epoch: 36 | Loss: 2.1827 | Val Loss: 2.1445\n",
      "Epoch: 37 | Loss: 2.1793 | Val Loss: 2.1532\n",
      "Epoch: 38 | Loss: 2.1723 | Val Loss: 2.1566\n",
      "Epoch: 39 | Loss: 2.1737 | Val Loss: 2.1653\n",
      "Epoch: 40 | Loss: 2.1700 | Val Loss: 2.1538\n",
      "Epoch: 41 | Loss: 2.1773 | Val Loss: 2.1673\n",
      "Epoch: 42 | Loss: 2.1558 | Val Loss: 2.1794\n",
      "Epoch: 43 | Loss: 2.1565 | Val Loss: 2.1511\n",
      "Epoch: 44 | Loss: 2.1585 | Val Loss: 2.1645\n",
      "Epoch: 45 | Loss: 2.1656 | Val Loss: 2.1703\n",
      "Epoch: 46 | Loss: 2.1540 | Val Loss: 2.1735\n",
      "Epoch: 47 | Loss: 2.1517 | Val Loss: 2.1547\n",
      "Epoch: 48 | Loss: 2.1486 | Val Loss: 2.1654\n",
      "Epoch: 49 | Loss: 2.1476 | Val Loss: 2.1658\n",
      "Epoch: 50 | Loss: 2.1442 | Val Loss: 2.2065\n",
      "Epoch: 51 | Loss: 2.1350 | Val Loss: 2.1336\n",
      "Epoch: 52 | Loss: 2.1413 | Val Loss: 2.1336\n",
      "Epoch: 53 | Loss: 2.1415 | Val Loss: 2.2011\n",
      "Epoch: 54 | Loss: 2.1331 | Val Loss: 2.1427\n",
      "Epoch: 55 | Loss: 2.1301 | Val Loss: 2.1456\n",
      "Epoch: 56 | Loss: 2.1325 | Val Loss: 2.1330\n",
      "Epoch: 57 | Loss: 2.1263 | Val Loss: 2.1784\n",
      "Epoch: 58 | Loss: 2.1279 | Val Loss: 2.1681\n",
      "Epoch: 59 | Loss: 2.1195 | Val Loss: 2.1329\n",
      "Epoch: 60 | Loss: 2.1168 | Val Loss: 2.1390\n",
      "Epoch: 61 | Loss: 2.1182 | Val Loss: 2.1777\n",
      "Epoch: 62 | Loss: 2.1150 | Val Loss: 2.1508\n",
      "Epoch: 63 | Loss: 2.1098 | Val Loss: 2.1393\n",
      "Epoch: 64 | Loss: 2.1087 | Val Loss: 2.1647\n",
      "Epoch: 65 | Loss: 2.1155 | Val Loss: 2.1440\n",
      "Epoch: 66 | Loss: 2.1015 | Val Loss: 2.1666\n",
      "Epoch: 67 | Loss: 2.1057 | Val Loss: 2.1178\n",
      "Epoch: 68 | Loss: 2.1035 | Val Loss: 2.1295\n",
      "Epoch: 69 | Loss: 2.1071 | Val Loss: 2.1364\n",
      "Epoch: 70 | Loss: 2.1031 | Val Loss: 2.1246\n",
      "Epoch: 71 | Loss: 2.0988 | Val Loss: 2.1274\n",
      "Epoch: 72 | Loss: 2.0999 | Val Loss: 2.1421\n",
      "Epoch: 73 | Loss: 2.0946 | Val Loss: 2.1297\n",
      "Epoch: 74 | Loss: 2.0951 | Val Loss: 2.1253\n",
      "Epoch: 75 | Loss: 2.0804 | Val Loss: 2.1330\n",
      "Epoch: 76 | Loss: 2.0838 | Val Loss: 2.1264\n",
      "Epoch: 77 | Loss: 2.0931 | Val Loss: 2.1440\n",
      "Epoch: 78 | Loss: 2.0804 | Val Loss: 2.1511\n",
      "Epoch: 79 | Loss: 2.0821 | Val Loss: 2.1150\n",
      "Epoch: 80 | Loss: 2.0808 | Val Loss: 2.1331\n",
      "Epoch: 81 | Loss: 2.0733 | Val Loss: 2.1326\n",
      "Epoch: 82 | Loss: 2.0709 | Val Loss: 2.1252\n",
      "Epoch: 83 | Loss: 2.0753 | Val Loss: 2.2001\n",
      "Epoch: 84 | Loss: 2.0778 | Val Loss: 2.1195\n",
      "Epoch: 85 | Loss: 2.0685 | Val Loss: 2.1364\n",
      "Epoch: 86 | Loss: 2.0637 | Val Loss: 2.1090\n",
      "Epoch: 87 | Loss: 2.0599 | Val Loss: 2.1190\n",
      "Epoch: 88 | Loss: 2.0622 | Val Loss: 2.1132\n",
      "Epoch: 89 | Loss: 2.0619 | Val Loss: 2.1145\n",
      "Epoch: 90 | Loss: 2.0524 | Val Loss: 2.1408\n",
      "Epoch: 91 | Loss: 2.0567 | Val Loss: 2.1326\n",
      "Epoch: 92 | Loss: 2.0561 | Val Loss: 2.1264\n",
      "Epoch: 93 | Loss: 2.0570 | Val Loss: 2.1238\n",
      "Epoch: 94 | Loss: 2.0526 | Val Loss: 2.1376\n",
      "Epoch: 95 | Loss: 2.0513 | Val Loss: 2.1764\n",
      "Epoch: 96 | Loss: 2.0471 | Val Loss: 2.1460\n",
      "Epoch: 97 | Loss: 2.0443 | Val Loss: 2.1265\n",
      "Epoch: 98 | Loss: 2.0421 | Val Loss: 2.1405\n",
      "Epoch: 99 | Loss: 2.0411 | Val Loss: 2.1332\n",
      "Epoch: 100 | Loss: 2.0365 | Val Loss: 2.1578\n",
      "Epoch: 101 | Loss: 2.0337 | Val Loss: 2.1237\n",
      "Epoch: 102 | Loss: 2.0359 | Val Loss: 2.1149\n",
      "Epoch: 103 | Loss: 2.0293 | Val Loss: 2.1052\n",
      "Epoch: 104 | Loss: 2.0355 | Val Loss: 2.1154\n",
      "Epoch: 105 | Loss: 2.0292 | Val Loss: 2.1185\n",
      "Epoch: 106 | Loss: 2.0286 | Val Loss: 2.1533\n",
      "Epoch: 107 | Loss: 2.0291 | Val Loss: 2.1079\n",
      "Epoch: 108 | Loss: 2.0304 | Val Loss: 2.0909\n",
      "Epoch: 109 | Loss: 2.0154 | Val Loss: 2.1083\n",
      "Epoch: 110 | Loss: 2.0189 | Val Loss: 2.1113\n",
      "Epoch: 111 | Loss: 2.0193 | Val Loss: 2.0913\n",
      "Epoch: 112 | Loss: 2.0106 | Val Loss: 2.1250\n",
      "Epoch: 113 | Loss: 2.0196 | Val Loss: 2.1117\n",
      "Epoch: 114 | Loss: 2.0155 | Val Loss: 2.1054\n",
      "Epoch: 115 | Loss: 2.0121 | Val Loss: 2.1034\n",
      "Epoch: 116 | Loss: 2.0039 | Val Loss: 2.1089\n",
      "Epoch: 117 | Loss: 2.0019 | Val Loss: 2.0978\n",
      "Epoch: 118 | Loss: 2.0008 | Val Loss: 2.1589\n",
      "Epoch: 119 | Loss: 2.0015 | Val Loss: 2.1033\n",
      "Epoch: 120 | Loss: 1.9968 | Val Loss: 2.1344\n",
      "Epoch: 121 | Loss: 1.9928 | Val Loss: 2.1120\n",
      "Epoch: 122 | Loss: 2.0038 | Val Loss: 2.1216\n",
      "Epoch: 123 | Loss: 1.9954 | Val Loss: 2.1054\n",
      "Epoch: 124 | Loss: 1.9929 | Val Loss: 2.1241\n",
      "Epoch: 125 | Loss: 1.9878 | Val Loss: 2.0868\n",
      "Epoch: 126 | Loss: 1.9903 | Val Loss: 2.0916\n",
      "Epoch: 127 | Loss: 1.9889 | Val Loss: 2.0865\n",
      "Epoch: 128 | Loss: 1.9805 | Val Loss: 2.0891\n",
      "Epoch: 129 | Loss: 1.9821 | Val Loss: 2.1052\n",
      "Epoch: 130 | Loss: 1.9821 | Val Loss: 2.0998\n",
      "Epoch: 131 | Loss: 1.9879 | Val Loss: 2.1245\n",
      "Epoch: 132 | Loss: 1.9758 | Val Loss: 2.0983\n",
      "Epoch: 133 | Loss: 1.9809 | Val Loss: 2.1063\n",
      "Epoch: 134 | Loss: 1.9813 | Val Loss: 2.0904\n",
      "Epoch: 135 | Loss: 1.9760 | Val Loss: 2.1033\n",
      "Epoch: 136 | Loss: 1.9731 | Val Loss: 2.1122\n",
      "Epoch: 137 | Loss: 1.9666 | Val Loss: 2.0825\n",
      "Epoch: 138 | Loss: 1.9676 | Val Loss: 2.1007\n",
      "Epoch: 139 | Loss: 1.9744 | Val Loss: 2.1001\n",
      "Epoch: 140 | Loss: 1.9635 | Val Loss: 2.1136\n",
      "Epoch: 141 | Loss: 1.9621 | Val Loss: 2.1218\n",
      "Epoch: 142 | Loss: 1.9611 | Val Loss: 2.0904\n",
      "Epoch: 143 | Loss: 1.9657 | Val Loss: 2.0996\n",
      "Epoch: 144 | Loss: 1.9586 | Val Loss: 2.1010\n",
      "Epoch: 145 | Loss: 1.9543 | Val Loss: 2.1141\n",
      "Epoch: 146 | Loss: 1.9462 | Val Loss: 2.0950\n",
      "Epoch: 147 | Loss: 1.9554 | Val Loss: 2.1009\n",
      "Epoch: 148 | Loss: 1.9525 | Val Loss: 2.1299\n",
      "Epoch: 149 | Loss: 1.9446 | Val Loss: 2.1340\n",
      "Epoch: 150 | Loss: 1.9521 | Val Loss: 2.1132\n",
      "Epoch: 151 | Loss: 1.9380 | Val Loss: 2.0983\n",
      "Epoch: 152 | Loss: 1.9451 | Val Loss: 2.0909\n",
      "Epoch: 153 | Loss: 1.9415 | Val Loss: 2.1102\n",
      "Epoch: 154 | Loss: 1.9395 | Val Loss: 2.1391\n",
      "Epoch: 155 | Loss: 1.9315 | Val Loss: 2.0978\n",
      "Epoch: 156 | Loss: 1.9342 | Val Loss: 2.0876\n",
      "Epoch: 157 | Loss: 1.9285 | Val Loss: 2.1366\n",
      "Epoch: 158 | Loss: 1.9375 | Val Loss: 2.0983\n",
      "Epoch: 159 | Loss: 1.9277 | Val Loss: 2.1077\n",
      "Epoch: 160 | Loss: 1.9308 | Val Loss: 2.1037\n",
      "Epoch: 161 | Loss: 1.9261 | Val Loss: 2.1055\n",
      "Epoch: 162 | Loss: 1.9301 | Val Loss: 2.1100\n",
      "Epoch: 163 | Loss: 1.9208 | Val Loss: 2.0781\n",
      "Epoch: 164 | Loss: 1.9244 | Val Loss: 2.0978\n",
      "Epoch: 165 | Loss: 1.9194 | Val Loss: 2.1052\n",
      "Epoch: 166 | Loss: 1.9107 | Val Loss: 2.0839\n",
      "Epoch: 167 | Loss: 1.9105 | Val Loss: 2.1080\n",
      "Epoch: 168 | Loss: 1.9146 | Val Loss: 2.0972\n",
      "Epoch: 169 | Loss: 1.9175 | Val Loss: 2.1421\n",
      "Epoch: 170 | Loss: 1.9104 | Val Loss: 2.0917\n",
      "Epoch: 171 | Loss: 1.9007 | Val Loss: 2.1116\n",
      "Epoch: 172 | Loss: 1.9035 | Val Loss: 2.1035\n",
      "Epoch: 173 | Loss: 1.9102 | Val Loss: 2.1092\n",
      "Epoch: 174 | Loss: 1.9023 | Val Loss: 2.1010\n",
      "Epoch: 175 | Loss: 1.8982 | Val Loss: 2.1022\n",
      "Epoch: 176 | Loss: 1.8979 | Val Loss: 2.1307\n",
      "Epoch: 177 | Loss: 1.8991 | Val Loss: 2.1063\n",
      "Epoch: 178 | Loss: 1.9018 | Val Loss: 2.1102\n",
      "Epoch: 179 | Loss: 1.8924 | Val Loss: 2.0974\n",
      "Epoch: 180 | Loss: 1.8901 | Val Loss: 2.1232\n",
      "Epoch: 181 | Loss: 1.8946 | Val Loss: 2.1326\n",
      "Epoch: 182 | Loss: 1.8872 | Val Loss: 2.1106\n",
      "Epoch: 183 | Loss: 1.8931 | Val Loss: 2.0985\n",
      "Epoch: 184 | Loss: 1.8761 | Val Loss: 2.1258\n",
      "Epoch: 185 | Loss: 1.8854 | Val Loss: 2.1026\n",
      "Epoch: 186 | Loss: 1.8788 | Val Loss: 2.1372\n",
      "Epoch: 187 | Loss: 1.8767 | Val Loss: 2.0999\n",
      "Epoch: 188 | Loss: 1.8727 | Val Loss: 2.1238\n",
      "Epoch: 189 | Loss: 1.8819 | Val Loss: 2.0891\n",
      "Epoch: 190 | Loss: 1.8784 | Val Loss: 2.1054\n",
      "Epoch: 191 | Loss: 1.8871 | Val Loss: 2.0877\n",
      "Epoch: 192 | Loss: 1.8738 | Val Loss: 2.1099\n",
      "Epoch: 193 | Loss: 1.8687 | Val Loss: 2.1236\n",
      "Epoch: 194 | Loss: 1.8655 | Val Loss: 2.1109\n"
     ]
    }
   ],
   "source": [
    "epochs_1 = 300\n",
    "model_fit_1 = train_dnn(epochs_1, 1, qs, 0.0, None, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 2.9667 | Val Loss: 2.3553\n",
      "Epoch: 2 | Loss: 2.5075 | Val Loss: 2.3631\n",
      "Epoch: 3 | Loss: 2.4477 | Val Loss: 2.3887\n",
      "Epoch: 4 | Loss: 2.3923 | Val Loss: 2.2201\n",
      "Epoch: 5 | Loss: 2.3396 | Val Loss: 2.1961\n",
      "Epoch: 6 | Loss: 2.3163 | Val Loss: 2.1986\n",
      "Epoch: 7 | Loss: 2.2999 | Val Loss: 2.1813\n",
      "Epoch: 8 | Loss: 2.2872 | Val Loss: 2.2721\n",
      "Epoch: 9 | Loss: 2.2830 | Val Loss: 2.1678\n",
      "Epoch: 10 | Loss: 2.2698 | Val Loss: 2.1462\n",
      "Epoch: 11 | Loss: 2.2460 | Val Loss: 2.1402\n",
      "Epoch: 12 | Loss: 2.2451 | Val Loss: 2.1749\n",
      "Epoch: 13 | Loss: 2.2327 | Val Loss: 2.2465\n",
      "Epoch: 14 | Loss: 2.2295 | Val Loss: 2.1367\n",
      "Epoch: 15 | Loss: 2.2174 | Val Loss: 2.2636\n",
      "Epoch: 16 | Loss: 2.2143 | Val Loss: 2.1650\n",
      "Epoch: 17 | Loss: 2.2170 | Val Loss: 2.1327\n",
      "Epoch: 18 | Loss: 2.1996 | Val Loss: 2.1294\n",
      "Epoch: 19 | Loss: 2.2009 | Val Loss: 2.1069\n",
      "Epoch: 20 | Loss: 2.2005 | Val Loss: 2.1268\n",
      "Epoch: 21 | Loss: 2.1833 | Val Loss: 2.1139\n",
      "Epoch: 22 | Loss: 2.1830 | Val Loss: 2.1492\n",
      "Epoch: 23 | Loss: 2.1811 | Val Loss: 2.2850\n",
      "Epoch: 24 | Loss: 2.1699 | Val Loss: 2.1026\n",
      "Epoch: 25 | Loss: 2.1775 | Val Loss: 2.1107\n",
      "Epoch: 26 | Loss: 2.1687 | Val Loss: 2.1171\n",
      "Epoch: 27 | Loss: 2.1688 | Val Loss: 2.1467\n",
      "Epoch: 28 | Loss: 2.1658 | Val Loss: 2.0938\n",
      "Epoch: 29 | Loss: 2.1557 | Val Loss: 2.1312\n",
      "Epoch: 30 | Loss: 2.1573 | Val Loss: 2.0846\n",
      "Epoch: 31 | Loss: 2.1644 | Val Loss: 2.1280\n",
      "Epoch: 32 | Loss: 2.1571 | Val Loss: 2.0919\n",
      "Epoch: 33 | Loss: 2.1579 | Val Loss: 2.0917\n",
      "Epoch: 34 | Loss: 2.1499 | Val Loss: 2.0660\n",
      "Epoch: 35 | Loss: 2.1440 | Val Loss: 2.0826\n",
      "Epoch: 36 | Loss: 2.1452 | Val Loss: 2.1222\n",
      "Epoch: 37 | Loss: 2.1498 | Val Loss: 2.0662\n",
      "Epoch: 38 | Loss: 2.1405 | Val Loss: 2.0995\n",
      "Epoch: 39 | Loss: 2.1434 | Val Loss: 2.0791\n",
      "Epoch: 40 | Loss: 2.1463 | Val Loss: 2.0806\n",
      "Epoch: 41 | Loss: 2.1317 | Val Loss: 2.1572\n",
      "Epoch: 42 | Loss: 2.1312 | Val Loss: 2.1526\n",
      "Epoch: 43 | Loss: 2.1343 | Val Loss: 2.0819\n",
      "Epoch: 44 | Loss: 2.1287 | Val Loss: 2.0808\n",
      "Epoch: 45 | Loss: 2.1244 | Val Loss: 2.0520\n",
      "Epoch: 46 | Loss: 2.1263 | Val Loss: 2.1150\n",
      "Epoch: 47 | Loss: 2.1199 | Val Loss: 2.0616\n",
      "Epoch: 48 | Loss: 2.1220 | Val Loss: 2.1297\n",
      "Epoch: 49 | Loss: 2.1192 | Val Loss: 2.0670\n",
      "Epoch: 50 | Loss: 2.1157 | Val Loss: 2.0679\n",
      "Epoch: 51 | Loss: 2.1253 | Val Loss: 2.0812\n",
      "Epoch: 52 | Loss: 2.1144 | Val Loss: 2.0766\n",
      "Epoch: 53 | Loss: 2.1161 | Val Loss: 2.0755\n",
      "Epoch: 54 | Loss: 2.1021 | Val Loss: 2.0643\n",
      "Epoch: 55 | Loss: 2.1146 | Val Loss: 2.0687\n",
      "Epoch: 56 | Loss: 2.1125 | Val Loss: 2.0742\n",
      "Epoch: 57 | Loss: 2.1085 | Val Loss: 2.0559\n",
      "Epoch: 58 | Loss: 2.0986 | Val Loss: 2.0432\n",
      "Epoch: 59 | Loss: 2.1020 | Val Loss: 2.0805\n",
      "Epoch: 60 | Loss: 2.0991 | Val Loss: 2.0718\n",
      "Epoch: 61 | Loss: 2.0959 | Val Loss: 2.1031\n",
      "Epoch: 62 | Loss: 2.0963 | Val Loss: 2.0713\n",
      "Epoch: 63 | Loss: 2.0981 | Val Loss: 2.0537\n",
      "Epoch: 64 | Loss: 2.0930 | Val Loss: 2.0748\n",
      "Epoch: 65 | Loss: 2.0933 | Val Loss: 2.0550\n",
      "Epoch: 66 | Loss: 2.0961 | Val Loss: 2.0803\n",
      "Epoch: 67 | Loss: 2.0871 | Val Loss: 2.0621\n",
      "Epoch: 68 | Loss: 2.0864 | Val Loss: 2.0476\n",
      "Epoch: 69 | Loss: 2.0864 | Val Loss: 2.0542\n",
      "Epoch: 70 | Loss: 2.0840 | Val Loss: 2.0610\n",
      "Epoch: 71 | Loss: 2.0807 | Val Loss: 2.0451\n",
      "Epoch: 72 | Loss: 2.0796 | Val Loss: 2.0580\n",
      "Epoch: 73 | Loss: 2.0827 | Val Loss: 2.0801\n",
      "Epoch: 74 | Loss: 2.0757 | Val Loss: 2.0358\n",
      "Epoch: 75 | Loss: 2.0764 | Val Loss: 2.0520\n",
      "Epoch: 76 | Loss: 2.0786 | Val Loss: 2.0612\n",
      "Epoch: 77 | Loss: 2.0786 | Val Loss: 2.0397\n",
      "Epoch: 78 | Loss: 2.0752 | Val Loss: 2.0407\n",
      "Epoch: 79 | Loss: 2.0674 | Val Loss: 2.0290\n",
      "Epoch: 80 | Loss: 2.0675 | Val Loss: 2.0285\n",
      "Epoch: 81 | Loss: 2.0702 | Val Loss: 2.0421\n",
      "Epoch: 82 | Loss: 2.0654 | Val Loss: 2.0868\n",
      "Epoch: 83 | Loss: 2.0640 | Val Loss: 2.0361\n",
      "Epoch: 84 | Loss: 2.0716 | Val Loss: 2.0409\n",
      "Epoch: 85 | Loss: 2.0633 | Val Loss: 2.0694\n",
      "Epoch: 86 | Loss: 2.0657 | Val Loss: 2.0381\n",
      "Epoch: 87 | Loss: 2.0593 | Val Loss: 2.0354\n",
      "Epoch: 88 | Loss: 2.0590 | Val Loss: 2.0463\n",
      "Epoch: 89 | Loss: 2.0633 | Val Loss: 2.0441\n",
      "Epoch: 90 | Loss: 2.0557 | Val Loss: 2.0469\n",
      "Epoch: 91 | Loss: 2.0520 | Val Loss: 2.0589\n",
      "Epoch: 92 | Loss: 2.0574 | Val Loss: 2.0262\n",
      "Epoch: 93 | Loss: 2.0517 | Val Loss: 2.0289\n",
      "Epoch: 94 | Loss: 2.0487 | Val Loss: 2.0400\n",
      "Epoch: 95 | Loss: 2.0480 | Val Loss: 2.0992\n",
      "Epoch: 96 | Loss: 2.0523 | Val Loss: 2.0356\n",
      "Epoch: 97 | Loss: 2.0527 | Val Loss: 2.0513\n",
      "Epoch: 98 | Loss: 2.0491 | Val Loss: 2.0542\n",
      "Epoch: 99 | Loss: 2.0447 | Val Loss: 2.0699\n",
      "Epoch: 100 | Loss: 2.0434 | Val Loss: 2.0535\n",
      "Epoch: 101 | Loss: 2.0384 | Val Loss: 2.0416\n",
      "Epoch: 102 | Loss: 2.0504 | Val Loss: 2.0228\n",
      "Epoch: 103 | Loss: 2.0396 | Val Loss: 2.0502\n",
      "Epoch: 104 | Loss: 2.0499 | Val Loss: 2.0187\n",
      "Epoch: 105 | Loss: 2.0374 | Val Loss: 2.0132\n",
      "Epoch: 106 | Loss: 2.0398 | Val Loss: 2.0400\n",
      "Epoch: 107 | Loss: 2.0362 | Val Loss: 2.0191\n",
      "Epoch: 108 | Loss: 2.0338 | Val Loss: 2.0561\n",
      "Epoch: 109 | Loss: 2.0312 | Val Loss: 2.0165\n",
      "Epoch: 110 | Loss: 2.0348 | Val Loss: 2.0418\n",
      "Epoch: 111 | Loss: 2.0284 | Val Loss: 2.0289\n",
      "Epoch: 112 | Loss: 2.0389 | Val Loss: 2.0439\n",
      "Epoch: 113 | Loss: 2.0235 | Val Loss: 2.0643\n",
      "Epoch: 114 | Loss: 2.0216 | Val Loss: 2.0433\n",
      "Epoch: 115 | Loss: 2.0246 | Val Loss: 2.0165\n",
      "Epoch: 116 | Loss: 2.0262 | Val Loss: 2.0089\n",
      "Epoch: 117 | Loss: 2.0203 | Val Loss: 2.0236\n",
      "Epoch: 118 | Loss: 2.0262 | Val Loss: 2.0133\n",
      "Epoch: 119 | Loss: 2.0243 | Val Loss: 2.0292\n",
      "Epoch: 120 | Loss: 2.0163 | Val Loss: 2.0228\n",
      "Epoch: 121 | Loss: 2.0204 | Val Loss: 2.0213\n",
      "Epoch: 122 | Loss: 2.0220 | Val Loss: 2.0079\n",
      "Epoch: 123 | Loss: 2.0165 | Val Loss: 2.0469\n",
      "Epoch: 124 | Loss: 2.0209 | Val Loss: 2.0339\n",
      "Epoch: 125 | Loss: 2.0142 | Val Loss: 2.0346\n",
      "Epoch: 126 | Loss: 2.0060 | Val Loss: 2.0013\n",
      "Epoch: 127 | Loss: 2.0074 | Val Loss: 2.0285\n",
      "Epoch: 128 | Loss: 2.0105 | Val Loss: 2.0400\n",
      "Epoch: 129 | Loss: 2.0058 | Val Loss: 2.0378\n",
      "Epoch: 130 | Loss: 2.0024 | Val Loss: 2.0418\n",
      "Epoch: 131 | Loss: 2.0019 | Val Loss: 2.0262\n",
      "Epoch: 132 | Loss: 2.0067 | Val Loss: 2.0363\n",
      "Epoch: 133 | Loss: 2.0064 | Val Loss: 2.0288\n",
      "Epoch: 134 | Loss: 1.9985 | Val Loss: 2.0154\n",
      "Epoch: 135 | Loss: 2.0065 | Val Loss: 2.0164\n",
      "Epoch: 136 | Loss: 2.0045 | Val Loss: 2.0077\n",
      "Epoch: 137 | Loss: 1.9903 | Val Loss: 2.0200\n",
      "Epoch: 138 | Loss: 1.9980 | Val Loss: 2.0090\n",
      "Epoch: 139 | Loss: 1.9985 | Val Loss: 2.0464\n",
      "Epoch: 140 | Loss: 1.9959 | Val Loss: 2.0248\n",
      "Epoch: 141 | Loss: 1.9952 | Val Loss: 2.0062\n",
      "Epoch: 142 | Loss: 1.9973 | Val Loss: 2.0272\n",
      "Epoch: 143 | Loss: 1.9931 | Val Loss: 2.0173\n",
      "Epoch: 144 | Loss: 1.9920 | Val Loss: 2.0087\n",
      "Epoch: 145 | Loss: 1.9926 | Val Loss: 2.0270\n",
      "Epoch: 146 | Loss: 1.9844 | Val Loss: 2.0851\n",
      "Epoch: 147 | Loss: 1.9830 | Val Loss: 2.0209\n",
      "Epoch: 148 | Loss: 1.9892 | Val Loss: 2.0270\n",
      "Epoch: 149 | Loss: 1.9862 | Val Loss: 2.0210\n",
      "Epoch: 150 | Loss: 1.9878 | Val Loss: 2.0430\n",
      "Epoch: 151 | Loss: 1.9894 | Val Loss: 2.0070\n",
      "Epoch: 152 | Loss: 1.9868 | Val Loss: 2.0100\n",
      "Epoch: 153 | Loss: 1.9828 | Val Loss: 2.0111\n",
      "Epoch: 154 | Loss: 1.9769 | Val Loss: 2.0404\n",
      "Epoch: 155 | Loss: 1.9753 | Val Loss: 1.9995\n",
      "Epoch: 156 | Loss: 1.9784 | Val Loss: 2.0168\n",
      "Epoch: 157 | Loss: 1.9760 | Val Loss: 2.0585\n",
      "Epoch: 158 | Loss: 1.9747 | Val Loss: 2.0045\n",
      "Epoch: 159 | Loss: 1.9751 | Val Loss: 2.0177\n",
      "Epoch: 160 | Loss: 1.9726 | Val Loss: 2.0057\n",
      "Epoch: 161 | Loss: 1.9742 | Val Loss: 2.0065\n",
      "Epoch: 162 | Loss: 1.9753 | Val Loss: 1.9923\n",
      "Epoch: 163 | Loss: 1.9623 | Val Loss: 2.0287\n",
      "Epoch: 164 | Loss: 1.9755 | Val Loss: 2.0217\n",
      "Epoch: 165 | Loss: 1.9723 | Val Loss: 2.0235\n",
      "Epoch: 166 | Loss: 1.9657 | Val Loss: 2.0023\n",
      "Epoch: 167 | Loss: 1.9709 | Val Loss: 1.9904\n",
      "Epoch: 168 | Loss: 1.9650 | Val Loss: 1.9952\n",
      "Epoch: 169 | Loss: 1.9649 | Val Loss: 2.0074\n",
      "Epoch: 170 | Loss: 1.9567 | Val Loss: 2.0232\n",
      "Epoch: 171 | Loss: 1.9610 | Val Loss: 2.0254\n",
      "Epoch: 172 | Loss: 1.9646 | Val Loss: 2.0019\n",
      "Epoch: 173 | Loss: 1.9640 | Val Loss: 2.0141\n",
      "Epoch: 174 | Loss: 1.9539 | Val Loss: 2.0226\n",
      "Epoch: 175 | Loss: 1.9569 | Val Loss: 1.9999\n",
      "Epoch: 176 | Loss: 1.9533 | Val Loss: 2.0262\n",
      "Epoch: 177 | Loss: 1.9482 | Val Loss: 2.0158\n",
      "Epoch: 178 | Loss: 1.9485 | Val Loss: 2.0111\n",
      "Epoch: 179 | Loss: 1.9507 | Val Loss: 2.0262\n",
      "Epoch: 180 | Loss: 1.9615 | Val Loss: 1.9832\n",
      "Epoch: 181 | Loss: 1.9600 | Val Loss: 2.0096\n",
      "Epoch: 182 | Loss: 1.9471 | Val Loss: 2.0275\n",
      "Epoch: 183 | Loss: 1.9619 | Val Loss: 2.0024\n",
      "Epoch: 184 | Loss: 1.9443 | Val Loss: 1.9992\n",
      "Epoch: 185 | Loss: 1.9513 | Val Loss: 1.9974\n",
      "Epoch: 186 | Loss: 1.9465 | Val Loss: 2.0064\n",
      "Epoch: 187 | Loss: 1.9486 | Val Loss: 2.0124\n",
      "Epoch: 188 | Loss: 1.9446 | Val Loss: 2.0235\n",
      "Epoch: 189 | Loss: 1.9509 | Val Loss: 1.9965\n",
      "Epoch: 190 | Loss: 1.9423 | Val Loss: 1.9840\n",
      "Epoch: 191 | Loss: 1.9460 | Val Loss: 2.0082\n",
      "Epoch: 192 | Loss: 1.9338 | Val Loss: 2.0132\n",
      "Epoch: 193 | Loss: 1.9385 | Val Loss: 2.0125\n",
      "Epoch: 194 | Loss: 1.9249 | Val Loss: 2.0238\n",
      "Epoch: 195 | Loss: 1.9339 | Val Loss: 2.0055\n",
      "Epoch: 196 | Loss: 1.9345 | Val Loss: 2.0054\n",
      "Epoch: 197 | Loss: 1.9384 | Val Loss: 2.0085\n",
      "Epoch: 198 | Loss: 1.9331 | Val Loss: 2.0013\n",
      "Epoch: 199 | Loss: 1.9287 | Val Loss: 2.0551\n",
      "Epoch: 200 | Loss: 1.9247 | Val Loss: 2.0138\n",
      "Epoch: 201 | Loss: 1.9266 | Val Loss: 1.9959\n",
      "Epoch: 202 | Loss: 1.9363 | Val Loss: 2.0048\n",
      "Epoch: 203 | Loss: 1.9359 | Val Loss: 1.9978\n",
      "Epoch: 204 | Loss: 1.9401 | Val Loss: 2.0201\n",
      "Epoch: 205 | Loss: 1.9277 | Val Loss: 2.0036\n",
      "Epoch: 206 | Loss: 1.9254 | Val Loss: 2.0061\n",
      "Epoch: 207 | Loss: 1.9314 | Val Loss: 1.9940\n",
      "Epoch: 208 | Loss: 1.9169 | Val Loss: 2.0256\n",
      "Epoch: 209 | Loss: 1.9205 | Val Loss: 2.0060\n",
      "Epoch: 210 | Loss: 1.9128 | Val Loss: 2.0184\n",
      "Epoch: 211 | Loss: 1.9231 | Val Loss: 1.9962\n",
      "Epoch: 212 | Loss: 1.9168 | Val Loss: 2.0195\n",
      "Epoch: 213 | Loss: 1.9168 | Val Loss: 2.0011\n",
      "Epoch: 214 | Loss: 1.9218 | Val Loss: 2.0160\n",
      "Epoch: 215 | Loss: 1.9273 | Val Loss: 2.0182\n",
      "Epoch: 216 | Loss: 1.9144 | Val Loss: 2.0180\n",
      "Epoch: 217 | Loss: 1.9201 | Val Loss: 1.9970\n",
      "Epoch: 218 | Loss: 1.9122 | Val Loss: 1.9839\n",
      "Epoch: 219 | Loss: 1.9156 | Val Loss: 2.0324\n",
      "Epoch: 220 | Loss: 1.9120 | Val Loss: 1.9933\n",
      "Epoch: 221 | Loss: 1.9150 | Val Loss: 2.0056\n",
      "Epoch: 222 | Loss: 1.9170 | Val Loss: 2.0072\n",
      "Epoch: 223 | Loss: 1.9035 | Val Loss: 2.0057\n",
      "Epoch: 224 | Loss: 1.9135 | Val Loss: 2.0054\n",
      "Epoch: 225 | Loss: 1.9123 | Val Loss: 2.0171\n",
      "Epoch: 226 | Loss: 1.9031 | Val Loss: 2.0290\n",
      "Epoch: 227 | Loss: 1.9076 | Val Loss: 2.0033\n",
      "Epoch: 228 | Loss: 1.9059 | Val Loss: 2.0090\n",
      "Epoch: 229 | Loss: 1.9177 | Val Loss: 2.0313\n",
      "Epoch: 230 | Loss: 1.8998 | Val Loss: 2.0081\n",
      "Epoch: 231 | Loss: 1.9043 | Val Loss: 1.9956\n",
      "Epoch: 232 | Loss: 1.9022 | Val Loss: 1.9928\n",
      "Epoch: 233 | Loss: 1.9108 | Val Loss: 2.0029\n",
      "Epoch: 234 | Loss: 1.9067 | Val Loss: 2.0132\n",
      "Epoch: 235 | Loss: 1.8928 | Val Loss: 1.9963\n",
      "Epoch: 236 | Loss: 1.8986 | Val Loss: 2.0032\n",
      "Epoch: 237 | Loss: 1.8980 | Val Loss: 2.0085\n",
      "Epoch: 238 | Loss: 1.8957 | Val Loss: 2.0131\n",
      "Epoch: 239 | Loss: 1.8975 | Val Loss: 1.9865\n",
      "Epoch: 240 | Loss: 1.8917 | Val Loss: 2.0267\n",
      "Epoch: 241 | Loss: 1.8890 | Val Loss: 1.9856\n",
      "Epoch: 242 | Loss: 1.8893 | Val Loss: 1.9877\n",
      "Epoch: 243 | Loss: 1.8852 | Val Loss: 2.0205\n",
      "Epoch: 244 | Loss: 1.8862 | Val Loss: 2.0147\n",
      "Epoch: 245 | Loss: 1.8924 | Val Loss: 1.9913\n",
      "Epoch: 246 | Loss: 1.8871 | Val Loss: 2.0023\n",
      "Epoch: 247 | Loss: 1.8793 | Val Loss: 2.0106\n",
      "Epoch: 248 | Loss: 1.8929 | Val Loss: 1.9926\n",
      "Epoch: 249 | Loss: 1.8823 | Val Loss: 1.9952\n",
      "Epoch: 250 | Loss: 1.8867 | Val Loss: 2.0107\n",
      "Epoch: 251 | Loss: 1.8864 | Val Loss: 2.0037\n",
      "Epoch: 252 | Loss: 1.8850 | Val Loss: 2.0024\n",
      "Epoch: 253 | Loss: 1.8829 | Val Loss: 1.9977\n",
      "Epoch: 254 | Loss: 1.8754 | Val Loss: 1.9956\n",
      "Epoch: 255 | Loss: 1.8826 | Val Loss: 2.0015\n",
      "Epoch: 256 | Loss: 1.8741 | Val Loss: 2.0011\n",
      "Epoch: 257 | Loss: 1.8828 | Val Loss: 1.9880\n",
      "Epoch: 258 | Loss: 1.8813 | Val Loss: 2.0177\n",
      "Epoch: 259 | Loss: 1.8791 | Val Loss: 2.0116\n",
      "Epoch: 260 | Loss: 1.8766 | Val Loss: 2.0214\n",
      "Epoch: 261 | Loss: 1.8733 | Val Loss: 2.0007\n",
      "Epoch: 262 | Loss: 1.8696 | Val Loss: 1.9878\n",
      "Epoch: 263 | Loss: 1.8783 | Val Loss: 2.0036\n",
      "Epoch: 264 | Loss: 1.8771 | Val Loss: 1.9941\n",
      "Epoch: 265 | Loss: 1.8757 | Val Loss: 2.0056\n",
      "Epoch: 266 | Loss: 1.8727 | Val Loss: 1.9845\n",
      "Epoch: 267 | Loss: 1.8768 | Val Loss: 1.9835\n",
      "Epoch: 268 | Loss: 1.8712 | Val Loss: 1.9957\n",
      "Epoch: 269 | Loss: 1.8612 | Val Loss: 2.0186\n",
      "Epoch: 270 | Loss: 1.8719 | Val Loss: 1.9994\n",
      "Epoch: 271 | Loss: 1.8811 | Val Loss: 1.9972\n",
      "Epoch: 272 | Loss: 1.8604 | Val Loss: 2.0257\n",
      "Epoch: 273 | Loss: 1.8714 | Val Loss: 2.0119\n",
      "Epoch: 274 | Loss: 1.8675 | Val Loss: 2.0065\n",
      "Epoch: 275 | Loss: 1.8651 | Val Loss: 2.0149\n",
      "Epoch: 276 | Loss: 1.8598 | Val Loss: 1.9973\n",
      "Epoch: 277 | Loss: 1.8574 | Val Loss: 1.9917\n",
      "Epoch: 278 | Loss: 1.8584 | Val Loss: 1.9849\n",
      "Epoch: 279 | Loss: 1.8559 | Val Loss: 2.0114\n",
      "Epoch: 280 | Loss: 1.8606 | Val Loss: 1.9861\n",
      "Epoch: 281 | Loss: 1.8636 | Val Loss: 1.9919\n",
      "Epoch: 282 | Loss: 1.8654 | Val Loss: 1.9932\n",
      "Epoch: 283 | Loss: 1.8559 | Val Loss: 1.9789\n",
      "Epoch: 284 | Loss: 1.8520 | Val Loss: 1.9932\n",
      "Epoch: 285 | Loss: 1.8557 | Val Loss: 2.0020\n",
      "Epoch: 286 | Loss: 1.8547 | Val Loss: 1.9861\n",
      "Epoch: 287 | Loss: 1.8550 | Val Loss: 2.0043\n",
      "Epoch: 288 | Loss: 1.8515 | Val Loss: 2.0029\n",
      "Epoch: 289 | Loss: 1.8512 | Val Loss: 2.0031\n",
      "Epoch: 290 | Loss: 1.8572 | Val Loss: 1.9952\n",
      "Epoch: 291 | Loss: 1.8495 | Val Loss: 1.9845\n",
      "Epoch: 292 | Loss: 1.8461 | Val Loss: 1.9848\n",
      "Epoch: 293 | Loss: 1.8519 | Val Loss: 1.9812\n",
      "Epoch: 294 | Loss: 1.8412 | Val Loss: 1.9772\n",
      "Epoch: 295 | Loss: 1.8489 | Val Loss: 1.9984\n",
      "Epoch: 296 | Loss: 1.8484 | Val Loss: 1.9906\n",
      "Epoch: 297 | Loss: 1.8458 | Val Loss: 1.9747\n",
      "Epoch: 298 | Loss: 1.8476 | Val Loss: 2.0243\n",
      "Epoch: 299 | Loss: 1.8458 | Val Loss: 1.9898\n",
      "Epoch: 300 | Loss: 1.8317 | Val Loss: 2.0040\n",
      "Epoch: 301 | Loss: 1.8399 | Val Loss: 2.0192\n",
      "Epoch: 302 | Loss: 1.8379 | Val Loss: 2.0071\n",
      "Epoch: 303 | Loss: 1.8407 | Val Loss: 2.0054\n",
      "Epoch: 304 | Loss: 1.8450 | Val Loss: 2.0081\n",
      "Epoch: 305 | Loss: 1.8418 | Val Loss: 1.9910\n",
      "Epoch: 306 | Loss: 1.8367 | Val Loss: 1.9774\n",
      "Epoch: 307 | Loss: 1.8443 | Val Loss: 1.9826\n",
      "Epoch: 308 | Loss: 1.8457 | Val Loss: 1.9881\n",
      "Epoch: 309 | Loss: 1.8322 | Val Loss: 1.9914\n",
      "Epoch: 310 | Loss: 1.8441 | Val Loss: 1.9800\n",
      "Epoch: 311 | Loss: 1.8395 | Val Loss: 2.0023\n",
      "Epoch: 312 | Loss: 1.8332 | Val Loss: 1.9795\n",
      "Epoch: 313 | Loss: 1.8359 | Val Loss: 1.9907\n",
      "Epoch: 314 | Loss: 1.8281 | Val Loss: 2.0103\n",
      "Epoch: 315 | Loss: 1.8384 | Val Loss: 2.0052\n",
      "Epoch: 316 | Loss: 1.8157 | Val Loss: 1.9954\n",
      "Epoch: 317 | Loss: 1.8301 | Val Loss: 2.0068\n",
      "Epoch: 318 | Loss: 1.8238 | Val Loss: 2.0046\n",
      "Epoch: 319 | Loss: 1.8352 | Val Loss: 2.0022\n",
      "Epoch: 320 | Loss: 1.8312 | Val Loss: 1.9717\n",
      "Epoch: 321 | Loss: 1.8421 | Val Loss: 2.0375\n",
      "Epoch: 322 | Loss: 1.8200 | Val Loss: 1.9939\n",
      "Epoch: 323 | Loss: 1.8208 | Val Loss: 2.0139\n",
      "Epoch: 324 | Loss: 1.8195 | Val Loss: 1.9842\n",
      "Epoch: 325 | Loss: 1.8262 | Val Loss: 1.9889\n",
      "Epoch: 326 | Loss: 1.8229 | Val Loss: 1.9880\n",
      "Epoch: 327 | Loss: 1.8330 | Val Loss: 1.9827\n",
      "Epoch: 328 | Loss: 1.8246 | Val Loss: 1.9959\n",
      "Epoch: 329 | Loss: 1.8278 | Val Loss: 1.9857\n",
      "Epoch: 330 | Loss: 1.8143 | Val Loss: 1.9950\n",
      "Epoch: 331 | Loss: 1.8168 | Val Loss: 1.9936\n",
      "Epoch: 332 | Loss: 1.8185 | Val Loss: 1.9903\n",
      "Epoch: 333 | Loss: 1.8154 | Val Loss: 1.9897\n",
      "Epoch: 334 | Loss: 1.8157 | Val Loss: 1.9904\n",
      "Epoch: 335 | Loss: 1.8260 | Val Loss: 2.0033\n",
      "Epoch: 336 | Loss: 1.8187 | Val Loss: 1.9778\n",
      "Epoch: 337 | Loss: 1.8178 | Val Loss: 2.0028\n",
      "Epoch: 338 | Loss: 1.8154 | Val Loss: 2.0033\n",
      "Epoch: 339 | Loss: 1.8088 | Val Loss: 1.9896\n",
      "Epoch: 340 | Loss: 1.8129 | Val Loss: 2.0075\n",
      "Epoch: 341 | Loss: 1.8005 | Val Loss: 2.0051\n",
      "Epoch: 342 | Loss: 1.8036 | Val Loss: 1.9960\n",
      "Epoch: 343 | Loss: 1.8133 | Val Loss: 1.9831\n",
      "Epoch: 344 | Loss: 1.8146 | Val Loss: 1.9887\n",
      "Epoch: 345 | Loss: 1.8081 | Val Loss: 1.9809\n",
      "Epoch: 346 | Loss: 1.8022 | Val Loss: 2.0248\n",
      "Epoch: 347 | Loss: 1.8164 | Val Loss: 1.9794\n",
      "Epoch: 348 | Loss: 1.8028 | Val Loss: 2.0163\n",
      "Epoch: 349 | Loss: 1.7924 | Val Loss: 2.0009\n",
      "Epoch: 350 | Loss: 1.8011 | Val Loss: 1.9765\n",
      "Epoch: 351 | Loss: 1.7985 | Val Loss: 2.0011\n",
      "Epoch: 352 | Loss: 1.8001 | Val Loss: 1.9873\n",
      "Epoch: 353 | Loss: 1.8005 | Val Loss: 1.9876\n",
      "Epoch: 354 | Loss: 1.7965 | Val Loss: 2.0052\n",
      "Epoch: 355 | Loss: 1.8018 | Val Loss: 1.9884\n",
      "Epoch: 356 | Loss: 1.8089 | Val Loss: 1.9847\n",
      "Epoch: 357 | Loss: 1.7982 | Val Loss: 1.9804\n",
      "Epoch: 358 | Loss: 1.7973 | Val Loss: 1.9883\n",
      "Epoch: 359 | Loss: 1.7935 | Val Loss: 1.9789\n",
      "Epoch: 360 | Loss: 1.7883 | Val Loss: 1.9921\n",
      "Epoch: 361 | Loss: 1.7961 | Val Loss: 1.9778\n",
      "Epoch: 362 | Loss: 1.7980 | Val Loss: 2.0381\n",
      "Epoch: 363 | Loss: 1.7966 | Val Loss: 1.9979\n",
      "Epoch: 364 | Loss: 1.7954 | Val Loss: 2.0063\n",
      "Epoch: 365 | Loss: 1.7925 | Val Loss: 1.9965\n",
      "Epoch: 366 | Loss: 1.8001 | Val Loss: 1.9923\n",
      "Epoch: 367 | Loss: 1.8002 | Val Loss: 1.9662\n",
      "Epoch: 368 | Loss: 1.7916 | Val Loss: 2.0087\n",
      "Epoch: 369 | Loss: 1.7948 | Val Loss: 2.0037\n",
      "Epoch: 370 | Loss: 1.7773 | Val Loss: 1.9938\n",
      "Epoch: 371 | Loss: 1.7900 | Val Loss: 1.9887\n",
      "Epoch: 372 | Loss: 1.7924 | Val Loss: 1.9906\n",
      "Epoch: 373 | Loss: 1.7892 | Val Loss: 1.9871\n",
      "Epoch: 374 | Loss: 1.7843 | Val Loss: 2.0017\n",
      "Epoch: 375 | Loss: 1.7852 | Val Loss: 2.0142\n",
      "Epoch: 376 | Loss: 1.7852 | Val Loss: 1.9892\n",
      "Epoch: 377 | Loss: 1.7781 | Val Loss: 1.9997\n",
      "Epoch: 378 | Loss: 1.7891 | Val Loss: 1.9757\n",
      "Epoch: 379 | Loss: 1.7930 | Val Loss: 1.9798\n",
      "Epoch: 380 | Loss: 1.7755 | Val Loss: 1.9893\n",
      "Epoch: 381 | Loss: 1.7861 | Val Loss: 1.9745\n",
      "Epoch: 382 | Loss: 1.8009 | Val Loss: 1.9844\n",
      "Epoch: 383 | Loss: 1.7776 | Val Loss: 1.9967\n",
      "Epoch: 384 | Loss: 1.7760 | Val Loss: 1.9974\n",
      "Epoch: 385 | Loss: 1.7845 | Val Loss: 1.9856\n",
      "Epoch: 386 | Loss: 1.7834 | Val Loss: 2.0007\n",
      "Epoch: 387 | Loss: 1.7728 | Val Loss: 1.9565\n",
      "Epoch: 388 | Loss: 1.7796 | Val Loss: 2.0089\n",
      "Epoch: 389 | Loss: 1.7957 | Val Loss: 1.9820\n",
      "Epoch: 390 | Loss: 1.7663 | Val Loss: 2.0065\n",
      "Epoch: 391 | Loss: 1.7743 | Val Loss: 2.0011\n",
      "Epoch: 392 | Loss: 1.7709 | Val Loss: 2.0250\n",
      "Epoch: 393 | Loss: 1.7677 | Val Loss: 1.9898\n",
      "Epoch: 394 | Loss: 1.7749 | Val Loss: 1.9849\n",
      "Epoch: 395 | Loss: 1.7661 | Val Loss: 1.9944\n",
      "Epoch: 396 | Loss: 1.7670 | Val Loss: 1.9828\n",
      "Epoch: 397 | Loss: 1.7655 | Val Loss: 2.0239\n",
      "Epoch: 398 | Loss: 1.7773 | Val Loss: 1.9930\n",
      "Epoch: 399 | Loss: 1.7727 | Val Loss: 2.0077\n",
      "Epoch: 400 | Loss: 1.7574 | Val Loss: 1.9858\n",
      "Epoch: 401 | Loss: 1.7563 | Val Loss: 2.0196\n",
      "Epoch: 402 | Loss: 1.7790 | Val Loss: 1.9790\n",
      "Epoch: 403 | Loss: 1.7663 | Val Loss: 1.9948\n",
      "Epoch: 404 | Loss: 1.7668 | Val Loss: 1.9908\n",
      "Epoch: 405 | Loss: 1.7552 | Val Loss: 1.9845\n",
      "Epoch: 406 | Loss: 1.7690 | Val Loss: 1.9817\n",
      "Epoch: 407 | Loss: 1.7611 | Val Loss: 2.0449\n",
      "Epoch: 408 | Loss: 1.7651 | Val Loss: 1.9948\n",
      "Epoch: 409 | Loss: 1.7657 | Val Loss: 1.9903\n",
      "Epoch: 410 | Loss: 1.7663 | Val Loss: 1.9873\n",
      "Epoch: 411 | Loss: 1.7689 | Val Loss: 2.0193\n",
      "Epoch: 412 | Loss: 1.7698 | Val Loss: 1.9998\n",
      "Epoch: 413 | Loss: 1.7659 | Val Loss: 2.0020\n",
      "Epoch: 414 | Loss: 1.7528 | Val Loss: 2.0189\n",
      "Epoch: 415 | Loss: 1.7439 | Val Loss: 1.9951\n",
      "Epoch: 416 | Loss: 1.7551 | Val Loss: 1.9940\n",
      "Epoch: 417 | Loss: 1.7568 | Val Loss: 2.0143\n",
      "Epoch: 418 | Loss: 1.7646 | Val Loss: 1.9911\n",
      "Epoch: 419 | Loss: 1.7662 | Val Loss: 1.9869\n",
      "Epoch: 420 | Loss: 1.7463 | Val Loss: 1.9884\n",
      "Epoch: 421 | Loss: 1.7439 | Val Loss: 2.0103\n",
      "Epoch: 422 | Loss: 1.7534 | Val Loss: 1.9954\n",
      "Epoch: 423 | Loss: 1.7552 | Val Loss: 1.9986\n",
      "Epoch: 424 | Loss: 1.7576 | Val Loss: 1.9957\n",
      "Epoch: 425 | Loss: 1.7501 | Val Loss: 2.0170\n",
      "Epoch: 426 | Loss: 1.7459 | Val Loss: 1.9760\n",
      "Epoch: 427 | Loss: 1.7532 | Val Loss: 1.9662\n",
      "Epoch: 428 | Loss: 1.7495 | Val Loss: 1.9873\n",
      "Epoch: 429 | Loss: 1.7474 | Val Loss: 1.9881\n",
      "Epoch: 430 | Loss: 1.7586 | Val Loss: 1.9986\n",
      "Epoch: 431 | Loss: 1.7482 | Val Loss: 1.9941\n",
      "Epoch: 432 | Loss: 1.7523 | Val Loss: 1.9786\n",
      "Epoch: 433 | Loss: 1.7423 | Val Loss: 1.9808\n",
      "Epoch: 434 | Loss: 1.7430 | Val Loss: 2.0065\n",
      "Epoch: 435 | Loss: 1.7517 | Val Loss: 1.9803\n",
      "Epoch: 436 | Loss: 1.7503 | Val Loss: 1.9987\n",
      "Epoch: 437 | Loss: 1.7450 | Val Loss: 1.9819\n",
      "Epoch: 438 | Loss: 1.7417 | Val Loss: 1.9912\n",
      "Epoch: 439 | Loss: 1.7353 | Val Loss: 2.0128\n",
      "Epoch: 440 | Loss: 1.7398 | Val Loss: 1.9939\n",
      "Epoch: 441 | Loss: 1.7284 | Val Loss: 1.9822\n",
      "Epoch: 442 | Loss: 1.7384 | Val Loss: 1.9818\n",
      "Epoch: 443 | Loss: 1.7411 | Val Loss: 2.0370\n",
      "Epoch: 444 | Loss: 1.7509 | Val Loss: 1.9851\n",
      "Epoch: 445 | Loss: 1.7345 | Val Loss: 1.9839\n",
      "Epoch: 446 | Loss: 1.7385 | Val Loss: 1.9914\n",
      "Epoch: 447 | Loss: 1.7364 | Val Loss: 1.9852\n",
      "Epoch: 448 | Loss: 1.7437 | Val Loss: 1.9773\n",
      "Epoch: 449 | Loss: 1.7331 | Val Loss: 1.9813\n",
      "Epoch: 450 | Loss: 1.7416 | Val Loss: 1.9944\n",
      "Epoch: 451 | Loss: 1.7441 | Val Loss: 2.0008\n",
      "Epoch: 452 | Loss: 1.7225 | Val Loss: 1.9909\n",
      "Epoch: 453 | Loss: 1.7358 | Val Loss: 1.9607\n",
      "Epoch: 454 | Loss: 1.7264 | Val Loss: 2.0036\n",
      "Epoch: 455 | Loss: 1.7318 | Val Loss: 1.9756\n",
      "Epoch: 456 | Loss: 1.7402 | Val Loss: 1.9741\n",
      "Epoch: 457 | Loss: 1.7332 | Val Loss: 1.9813\n",
      "Epoch: 458 | Loss: 1.7409 | Val Loss: 2.0232\n",
      "Epoch: 459 | Loss: 1.7395 | Val Loss: 1.9868\n",
      "Epoch: 460 | Loss: 1.7343 | Val Loss: 1.9839\n",
      "Epoch: 461 | Loss: 1.7292 | Val Loss: 1.9937\n",
      "Epoch: 462 | Loss: 1.7258 | Val Loss: 2.0023\n",
      "Epoch: 463 | Loss: 1.7238 | Val Loss: 2.0029\n",
      "Epoch: 464 | Loss: 1.7302 | Val Loss: 2.0170\n",
      "Epoch: 465 | Loss: 1.7349 | Val Loss: 1.9833\n",
      "Epoch: 466 | Loss: 1.7310 | Val Loss: 2.0100\n",
      "Epoch: 467 | Loss: 1.7272 | Val Loss: 1.9936\n",
      "Epoch: 468 | Loss: 1.7279 | Val Loss: 1.9808\n",
      "Epoch: 469 | Loss: 1.7357 | Val Loss: 1.9886\n",
      "Epoch: 470 | Loss: 1.7360 | Val Loss: 1.9680\n",
      "Epoch: 471 | Loss: 1.7220 | Val Loss: 1.9849\n",
      "Epoch: 472 | Loss: 1.7126 | Val Loss: 2.0297\n",
      "Epoch: 473 | Loss: 1.7278 | Val Loss: 1.9803\n",
      "Epoch: 474 | Loss: 1.7304 | Val Loss: 2.0051\n",
      "Epoch: 475 | Loss: 1.7116 | Val Loss: 1.9841\n",
      "Epoch: 476 | Loss: 1.7202 | Val Loss: 1.9933\n",
      "Epoch: 477 | Loss: 1.7205 | Val Loss: 1.9838\n",
      "Epoch: 478 | Loss: 1.7183 | Val Loss: 1.9897\n",
      "Epoch: 479 | Loss: 1.7265 | Val Loss: 2.0000\n",
      "Epoch: 480 | Loss: 1.7171 | Val Loss: 1.9668\n",
      "Epoch: 481 | Loss: 1.7230 | Val Loss: 1.9934\n",
      "Epoch: 482 | Loss: 1.7205 | Val Loss: 1.9996\n",
      "Epoch: 483 | Loss: 1.7141 | Val Loss: 1.9823\n",
      "Epoch: 484 | Loss: 1.7109 | Val Loss: 1.9883\n",
      "Epoch: 485 | Loss: 1.7119 | Val Loss: 1.9769\n",
      "Epoch: 486 | Loss: 1.7152 | Val Loss: 1.9973\n",
      "Epoch: 487 | Loss: 1.7217 | Val Loss: 1.9797\n",
      "Epoch: 488 | Loss: 1.7036 | Val Loss: 1.9979\n",
      "Epoch: 489 | Loss: 1.7174 | Val Loss: 2.0216\n",
      "Epoch: 490 | Loss: 1.7055 | Val Loss: 1.9797\n",
      "Epoch: 491 | Loss: 1.7060 | Val Loss: 2.0010\n",
      "Epoch: 492 | Loss: 1.7153 | Val Loss: 1.9882\n",
      "Epoch: 493 | Loss: 1.7079 | Val Loss: 1.9741\n",
      "Epoch: 494 | Loss: 1.7024 | Val Loss: 1.9694\n",
      "Epoch: 495 | Loss: 1.7137 | Val Loss: 2.0223\n",
      "Epoch: 496 | Loss: 1.7147 | Val Loss: 1.9786\n",
      "Epoch: 497 | Loss: 1.7210 | Val Loss: 1.9847\n",
      "Epoch: 498 | Loss: 1.7146 | Val Loss: 1.9879\n",
      "Epoch: 499 | Loss: 1.7036 | Val Loss: 2.0033\n",
      "Epoch: 500 | Loss: 1.7020 | Val Loss: 1.9911\n"
     ]
    }
   ],
   "source": [
    "epochs_2 = 500\n",
    "model_fit_2 = train_dnn(epochs_2, 2, qs, 0.0, '2', train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentile별로 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TARGET 1\n",
    "qs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "epochs = 200\n",
    "\n",
    "model_tg_1 = []\n",
    "\n",
    "for q in qs:\n",
    "    print(f\"Model for target1, for q = {q}\")\n",
    "    model_fit_tp = train_dnn(epochs, 1, q, train_loader, val_loader)\n",
    "    model_tg_1.append(model_fit_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TARGET 2\n",
    "qs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "epochs = 200\n",
    "\n",
    "model_tg_2 = []\n",
    "\n",
    "for q in qs:\n",
    "    print(f\"Model for target1, for q = {q}\")\n",
    "    model_fit_tp = train_dnn(epochs, 2, q, train_loader, val_loader)\n",
    "    model_tg_2.append(model_fit_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [15.5273, 23.3396, 29.1010,  ..., 56.5765, 62.3381, 70.1121]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    result = model_fit_1(val_x_torch)\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    current_model = model_fit_1\n",
    "    test_pred = current_model(val_x_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GT</th>\n",
       "      <th>PRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.005604</td>\n",
       "      <td>2.952671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.641113</td>\n",
       "      <td>61.471474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.907784</td>\n",
       "      <td>31.791132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.658344</td>\n",
       "      <td>55.151176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5242</th>\n",
       "      <td>8.256253</td>\n",
       "      <td>68.292389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5243</th>\n",
       "      <td>71.866142</td>\n",
       "      <td>65.454193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244</th>\n",
       "      <td>50.395756</td>\n",
       "      <td>45.993767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5245</th>\n",
       "      <td>47.004353</td>\n",
       "      <td>48.154396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5246</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5247 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             GT       PRED\n",
       "0      6.005604   2.952671\n",
       "1     61.641113  61.471474\n",
       "2      0.000000   0.000864\n",
       "3     40.907784  31.791132\n",
       "4     32.658344  55.151176\n",
       "...         ...        ...\n",
       "5242   8.256253  68.292389\n",
       "5243  71.866142  65.454193\n",
       "5244  50.395756  45.993767\n",
       "5245  47.004353  48.154396\n",
       "5246   0.000000   0.000849\n",
       "\n",
       "[5247 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_col = np.array(test_pred.detach().to('cpu'))\n",
    "gt_col = np.array(test_y_torch.detach().to('cpu'))\n",
    "\n",
    "pd.DataFrame({'GT':gt_col, 'PRED':pred_col})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(base_dir, 'test')\n",
    "lists = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3888, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = []\n",
    "\n",
    "for i in range(81):\n",
    "    file_path = os.path.join(test_dir, str(i)+'.csv')\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp['Time'] = temp['Hour']*60 + temp['Minute']\n",
    "    temp = temp.loc[temp.Day == 6, :][['Time', 'DHI', 'DNI', 'WS', 'RH', 'T', 'TARGET']]\n",
    "    \n",
    "    df_test.append(temp)\n",
    "\n",
    "X_test = pd.concat(df_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = torch.tensor(scaled.transform(X_test.values)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = model_fit_1(X_test_scaled)\n",
    "result_2 = model_fit_2(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(base_dir, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>80.csv_Day8_21h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7772</th>\n",
       "      <td>80.csv_Day8_22h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7773</th>\n",
       "      <td>80.csv_Day8_22h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7774</th>\n",
       "      <td>80.csv_Day8_23h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7775</th>\n",
       "      <td>80.csv_Day8_23h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7776 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  q_0.1  q_0.2  q_0.3  q_0.4  q_0.5  q_0.6  q_0.7  \\\n",
       "0       0.csv_Day7_0h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1       0.csv_Day7_0h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2       0.csv_Day7_1h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3       0.csv_Day7_1h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4       0.csv_Day7_2h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...                  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "7771  80.csv_Day8_21h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7772  80.csv_Day8_22h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7773  80.csv_Day8_22h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7774  80.csv_Day8_23h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7775  80.csv_Day8_23h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      q_0.8  q_0.9  \n",
       "0       0.0    0.0  \n",
       "1       0.0    0.0  \n",
       "2       0.0    0.0  \n",
       "3       0.0    0.0  \n",
       "4       0.0    0.0  \n",
       "...     ...    ...  \n",
       "7771    0.0    0.0  \n",
       "7772    0.0    0.0  \n",
       "7773    0.0    0.0  \n",
       "7774    0.0    0.0  \n",
       "7775    0.0    0.0  \n",
       "\n",
       "[7776 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = result_1.detach().to('cpu')\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = result_2.detach().to('cpu')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tofill = submission.loc[submission.apply(lambda x: x['id'].split('.')[1].split('_')[-1].split('h')[0] in ['0', '1', '2', '3', '4', '20', '21', '22', '23'], axis = 1), 'q_0.1':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2916.0</td>\n",
       "      <td>2916.0</td>\n",
       "      <td>2916.0</td>\n",
       "      <td>2916.0</td>\n",
       "      <td>2916.0</td>\n",
       "      <td>2916.0</td>\n",
       "      <td>2916.0</td>\n",
       "      <td>2916.0</td>\n",
       "      <td>2916.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        q_0.1   q_0.2   q_0.3   q_0.4   q_0.5   q_0.6   q_0.7   q_0.8   q_0.9\n",
       "count  2916.0  2916.0  2916.0  2916.0  2916.0  2916.0  2916.0  2916.0  2916.0\n",
       "mean      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n",
       "std       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n",
       "min       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n",
       "25%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n",
       "50%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n",
       "75%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0\n",
       "max       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in tofill.columns:\n",
    "    tofill[col].values[:] = 0\n",
    "tofill.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.loc[submission.apply(lambda x: x['id'].split('.')[1].split('_')[-1].split('h')[0] in ['0', '1', '2', '3', '4', '20', '21', '22', '23'], axis = 1), 'q_0.1':] = tofill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.csv_Day8_0h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.csv_Day8_0h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.csv_Day8_1h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.csv_Day8_1h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.csv_Day8_2h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.csv_Day8_2h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.csv_Day8_3h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.csv_Day8_3h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.csv_Day8_4h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.csv_Day8_4h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.csv_Day8_5h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.csv_Day8_5h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.csv_Day8_6h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.csv_Day8_6h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.csv_Day8_7h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.csv_Day8_7h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300151</td>\n",
       "      <td>1.270625</td>\n",
       "      <td>2.146317</td>\n",
       "      <td>2.978137</td>\n",
       "      <td>4.042618</td>\n",
       "      <td>5.482762</td>\n",
       "      <td>7.406671</td>\n",
       "      <td>10.082305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.csv_Day8_8h00m</td>\n",
       "      <td>1.432783</td>\n",
       "      <td>2.633054</td>\n",
       "      <td>3.769745</td>\n",
       "      <td>4.745201</td>\n",
       "      <td>5.703817</td>\n",
       "      <td>6.826095</td>\n",
       "      <td>8.231979</td>\n",
       "      <td>10.008958</td>\n",
       "      <td>12.383879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.csv_Day8_8h30m</td>\n",
       "      <td>5.343328</td>\n",
       "      <td>7.990869</td>\n",
       "      <td>10.091530</td>\n",
       "      <td>11.803741</td>\n",
       "      <td>13.359106</td>\n",
       "      <td>14.958852</td>\n",
       "      <td>16.830540</td>\n",
       "      <td>19.102013</td>\n",
       "      <td>22.172709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.csv_Day8_9h00m</td>\n",
       "      <td>5.663530</td>\n",
       "      <td>9.336858</td>\n",
       "      <td>12.179561</td>\n",
       "      <td>14.379154</td>\n",
       "      <td>16.203976</td>\n",
       "      <td>18.009214</td>\n",
       "      <td>20.103397</td>\n",
       "      <td>22.552837</td>\n",
       "      <td>25.802599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.csv_Day8_9h30m</td>\n",
       "      <td>6.148239</td>\n",
       "      <td>9.699357</td>\n",
       "      <td>12.779929</td>\n",
       "      <td>15.267114</td>\n",
       "      <td>17.318939</td>\n",
       "      <td>19.318117</td>\n",
       "      <td>21.501045</td>\n",
       "      <td>23.925030</td>\n",
       "      <td>27.037796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.csv_Day8_10h00m</td>\n",
       "      <td>11.986189</td>\n",
       "      <td>16.095707</td>\n",
       "      <td>18.875391</td>\n",
       "      <td>20.845049</td>\n",
       "      <td>22.268957</td>\n",
       "      <td>23.558903</td>\n",
       "      <td>24.996302</td>\n",
       "      <td>26.457218</td>\n",
       "      <td>28.399811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.csv_Day8_10h30m</td>\n",
       "      <td>15.891973</td>\n",
       "      <td>20.234146</td>\n",
       "      <td>23.126108</td>\n",
       "      <td>25.178673</td>\n",
       "      <td>26.698450</td>\n",
       "      <td>28.125517</td>\n",
       "      <td>29.758921</td>\n",
       "      <td>31.390409</td>\n",
       "      <td>33.415977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.csv_Day8_11h00m</td>\n",
       "      <td>10.327908</td>\n",
       "      <td>13.115922</td>\n",
       "      <td>16.554531</td>\n",
       "      <td>19.730862</td>\n",
       "      <td>22.687449</td>\n",
       "      <td>25.903923</td>\n",
       "      <td>29.253040</td>\n",
       "      <td>32.502426</td>\n",
       "      <td>35.625893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.csv_Day8_11h30m</td>\n",
       "      <td>7.104239</td>\n",
       "      <td>10.450844</td>\n",
       "      <td>14.436875</td>\n",
       "      <td>17.864676</td>\n",
       "      <td>20.654200</td>\n",
       "      <td>23.175154</td>\n",
       "      <td>25.537235</td>\n",
       "      <td>27.791485</td>\n",
       "      <td>30.398262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.csv_Day8_12h00m</td>\n",
       "      <td>6.732735</td>\n",
       "      <td>10.027844</td>\n",
       "      <td>13.967161</td>\n",
       "      <td>17.376102</td>\n",
       "      <td>20.174734</td>\n",
       "      <td>22.710037</td>\n",
       "      <td>25.084667</td>\n",
       "      <td>27.326441</td>\n",
       "      <td>29.816206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.csv_Day8_12h30m</td>\n",
       "      <td>6.017349</td>\n",
       "      <td>8.560924</td>\n",
       "      <td>11.584283</td>\n",
       "      <td>14.066480</td>\n",
       "      <td>16.153896</td>\n",
       "      <td>18.553984</td>\n",
       "      <td>21.617031</td>\n",
       "      <td>25.286360</td>\n",
       "      <td>29.151119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.csv_Day8_13h00m</td>\n",
       "      <td>5.498895</td>\n",
       "      <td>7.953820</td>\n",
       "      <td>10.953141</td>\n",
       "      <td>13.614726</td>\n",
       "      <td>16.071417</td>\n",
       "      <td>18.956478</td>\n",
       "      <td>22.304693</td>\n",
       "      <td>25.936716</td>\n",
       "      <td>29.356791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.csv_Day8_13h30m</td>\n",
       "      <td>6.120110</td>\n",
       "      <td>9.457367</td>\n",
       "      <td>13.614205</td>\n",
       "      <td>17.221045</td>\n",
       "      <td>20.167755</td>\n",
       "      <td>22.674093</td>\n",
       "      <td>24.947075</td>\n",
       "      <td>27.095312</td>\n",
       "      <td>29.633959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.csv_Day8_14h00m</td>\n",
       "      <td>5.910623</td>\n",
       "      <td>9.032769</td>\n",
       "      <td>12.669636</td>\n",
       "      <td>15.845767</td>\n",
       "      <td>18.579498</td>\n",
       "      <td>21.187515</td>\n",
       "      <td>23.817436</td>\n",
       "      <td>26.375765</td>\n",
       "      <td>29.004784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.csv_Day8_14h30m</td>\n",
       "      <td>6.048379</td>\n",
       "      <td>9.009285</td>\n",
       "      <td>12.088729</td>\n",
       "      <td>14.676775</td>\n",
       "      <td>16.930937</td>\n",
       "      <td>19.375290</td>\n",
       "      <td>22.220922</td>\n",
       "      <td>25.307062</td>\n",
       "      <td>28.369768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.csv_Day8_15h00m</td>\n",
       "      <td>5.863964</td>\n",
       "      <td>8.840895</td>\n",
       "      <td>11.571934</td>\n",
       "      <td>13.723083</td>\n",
       "      <td>15.505540</td>\n",
       "      <td>17.528002</td>\n",
       "      <td>20.083557</td>\n",
       "      <td>23.035936</td>\n",
       "      <td>26.085009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.csv_Day8_15h30m</td>\n",
       "      <td>5.557597</td>\n",
       "      <td>9.234477</td>\n",
       "      <td>12.273590</td>\n",
       "      <td>14.566936</td>\n",
       "      <td>16.229383</td>\n",
       "      <td>17.603292</td>\n",
       "      <td>19.049118</td>\n",
       "      <td>20.520939</td>\n",
       "      <td>22.427521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.csv_Day8_16h00m</td>\n",
       "      <td>3.226089</td>\n",
       "      <td>5.435119</td>\n",
       "      <td>7.200201</td>\n",
       "      <td>8.540858</td>\n",
       "      <td>9.535121</td>\n",
       "      <td>10.483666</td>\n",
       "      <td>11.560196</td>\n",
       "      <td>12.717364</td>\n",
       "      <td>14.191191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.csv_Day8_16h30m</td>\n",
       "      <td>0.877641</td>\n",
       "      <td>1.713716</td>\n",
       "      <td>2.627084</td>\n",
       "      <td>3.389136</td>\n",
       "      <td>3.988616</td>\n",
       "      <td>4.654905</td>\n",
       "      <td>5.481454</td>\n",
       "      <td>6.449512</td>\n",
       "      <td>7.705956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.csv_Day8_17h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.csv_Day8_17h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.csv_Day8_18h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.csv_Day8_18h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.csv_Day8_19h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.csv_Day8_19h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.csv_Day8_20h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.csv_Day8_20h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.csv_Day8_21h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.csv_Day8_21h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.csv_Day8_22h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.csv_Day8_22h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.csv_Day8_23h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.csv_Day8_23h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id      q_0.1      q_0.2      q_0.3      q_0.4      q_0.5  \\\n",
       "48   0.csv_Day8_0h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "49   0.csv_Day8_0h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "50   0.csv_Day8_1h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "51   0.csv_Day8_1h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "52   0.csv_Day8_2h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "53   0.csv_Day8_2h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "54   0.csv_Day8_3h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "55   0.csv_Day8_3h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "56   0.csv_Day8_4h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "57   0.csv_Day8_4h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "58   0.csv_Day8_5h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "59   0.csv_Day8_5h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "60   0.csv_Day8_6h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "61   0.csv_Day8_6h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "62   0.csv_Day8_7h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "63   0.csv_Day8_7h30m   0.000000   0.300151   1.270625   2.146317   2.978137   \n",
       "64   0.csv_Day8_8h00m   1.432783   2.633054   3.769745   4.745201   5.703817   \n",
       "65   0.csv_Day8_8h30m   5.343328   7.990869  10.091530  11.803741  13.359106   \n",
       "66   0.csv_Day8_9h00m   5.663530   9.336858  12.179561  14.379154  16.203976   \n",
       "67   0.csv_Day8_9h30m   6.148239   9.699357  12.779929  15.267114  17.318939   \n",
       "68  0.csv_Day8_10h00m  11.986189  16.095707  18.875391  20.845049  22.268957   \n",
       "69  0.csv_Day8_10h30m  15.891973  20.234146  23.126108  25.178673  26.698450   \n",
       "70  0.csv_Day8_11h00m  10.327908  13.115922  16.554531  19.730862  22.687449   \n",
       "71  0.csv_Day8_11h30m   7.104239  10.450844  14.436875  17.864676  20.654200   \n",
       "72  0.csv_Day8_12h00m   6.732735  10.027844  13.967161  17.376102  20.174734   \n",
       "73  0.csv_Day8_12h30m   6.017349   8.560924  11.584283  14.066480  16.153896   \n",
       "74  0.csv_Day8_13h00m   5.498895   7.953820  10.953141  13.614726  16.071417   \n",
       "75  0.csv_Day8_13h30m   6.120110   9.457367  13.614205  17.221045  20.167755   \n",
       "76  0.csv_Day8_14h00m   5.910623   9.032769  12.669636  15.845767  18.579498   \n",
       "77  0.csv_Day8_14h30m   6.048379   9.009285  12.088729  14.676775  16.930937   \n",
       "78  0.csv_Day8_15h00m   5.863964   8.840895  11.571934  13.723083  15.505540   \n",
       "79  0.csv_Day8_15h30m   5.557597   9.234477  12.273590  14.566936  16.229383   \n",
       "80  0.csv_Day8_16h00m   3.226089   5.435119   7.200201   8.540858   9.535121   \n",
       "81  0.csv_Day8_16h30m   0.877641   1.713716   2.627084   3.389136   3.988616   \n",
       "82  0.csv_Day8_17h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "83  0.csv_Day8_17h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "84  0.csv_Day8_18h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "85  0.csv_Day8_18h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "86  0.csv_Day8_19h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "87  0.csv_Day8_19h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "88  0.csv_Day8_20h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "89  0.csv_Day8_20h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "90  0.csv_Day8_21h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "91  0.csv_Day8_21h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "92  0.csv_Day8_22h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "93  0.csv_Day8_22h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "94  0.csv_Day8_23h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "95  0.csv_Day8_23h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        q_0.6      q_0.7      q_0.8      q_0.9  \n",
       "48   0.000000   0.000000   0.000000   0.000000  \n",
       "49   0.000000   0.000000   0.000000   0.000000  \n",
       "50   0.000000   0.000000   0.000000   0.000000  \n",
       "51   0.000000   0.000000   0.000000   0.000000  \n",
       "52   0.000000   0.000000   0.000000   0.000000  \n",
       "53   0.000000   0.000000   0.000000   0.000000  \n",
       "54   0.000000   0.000000   0.000000   0.000000  \n",
       "55   0.000000   0.000000   0.000000   0.000000  \n",
       "56   0.000000   0.000000   0.000000   0.000000  \n",
       "57   0.000000   0.000000   0.000000   0.000000  \n",
       "58   0.000000   0.000000   0.000000   0.000000  \n",
       "59   0.000000   0.000000   0.000000   0.000000  \n",
       "60   0.000000   0.000000   0.000000   0.000000  \n",
       "61   0.000000   0.000000   0.000000   0.000000  \n",
       "62   0.000000   0.000000   0.000000   0.000000  \n",
       "63   4.042618   5.482762   7.406671  10.082305  \n",
       "64   6.826095   8.231979  10.008958  12.383879  \n",
       "65  14.958852  16.830540  19.102013  22.172709  \n",
       "66  18.009214  20.103397  22.552837  25.802599  \n",
       "67  19.318117  21.501045  23.925030  27.037796  \n",
       "68  23.558903  24.996302  26.457218  28.399811  \n",
       "69  28.125517  29.758921  31.390409  33.415977  \n",
       "70  25.903923  29.253040  32.502426  35.625893  \n",
       "71  23.175154  25.537235  27.791485  30.398262  \n",
       "72  22.710037  25.084667  27.326441  29.816206  \n",
       "73  18.553984  21.617031  25.286360  29.151119  \n",
       "74  18.956478  22.304693  25.936716  29.356791  \n",
       "75  22.674093  24.947075  27.095312  29.633959  \n",
       "76  21.187515  23.817436  26.375765  29.004784  \n",
       "77  19.375290  22.220922  25.307062  28.369768  \n",
       "78  17.528002  20.083557  23.035936  26.085009  \n",
       "79  17.603292  19.049118  20.520939  22.427521  \n",
       "80  10.483666  11.560196  12.717364  14.191191  \n",
       "81   4.654905   5.481454   6.449512   7.705956  \n",
       "82   0.000000   0.000000   0.000000   0.000000  \n",
       "83   0.000000   0.000000   0.000000   0.000000  \n",
       "84   0.000000   0.000000   0.000000   0.000000  \n",
       "85   0.000000   0.000000   0.000000   0.000000  \n",
       "86   0.000000   0.000000   0.000000   0.000000  \n",
       "87   0.000000   0.000000   0.000000   0.000000  \n",
       "88   0.000000   0.000000   0.000000   0.000000  \n",
       "89   0.000000   0.000000   0.000000   0.000000  \n",
       "90   0.000000   0.000000   0.000000   0.000000  \n",
       "91   0.000000   0.000000   0.000000   0.000000  \n",
       "92   0.000000   0.000000   0.000000   0.000000  \n",
       "93   0.000000   0.000000   0.000000   0.000000  \n",
       "94   0.000000   0.000000   0.000000   0.000000  \n",
       "95   0.000000   0.000000   0.000000   0.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.iloc[48:96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.321143</td>\n",
       "      <td>12.641466</td>\n",
       "      <td>15.259423</td>\n",
       "      <td>17.068478</td>\n",
       "      <td>18.518168</td>\n",
       "      <td>19.768997</td>\n",
       "      <td>20.917525</td>\n",
       "      <td>22.095411</td>\n",
       "      <td>23.340250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.374304</td>\n",
       "      <td>19.111445</td>\n",
       "      <td>21.897484</td>\n",
       "      <td>23.809573</td>\n",
       "      <td>25.310299</td>\n",
       "      <td>26.574215</td>\n",
       "      <td>27.706749</td>\n",
       "      <td>28.869001</td>\n",
       "      <td>30.088685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242156</td>\n",
       "      <td>0.757101</td>\n",
       "      <td>1.285631</td>\n",
       "      <td>1.912275</td>\n",
       "      <td>2.569417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.262892</td>\n",
       "      <td>20.672598</td>\n",
       "      <td>26.426271</td>\n",
       "      <td>30.780162</td>\n",
       "      <td>34.629218</td>\n",
       "      <td>37.537903</td>\n",
       "      <td>40.414082</td>\n",
       "      <td>43.398237</td>\n",
       "      <td>46.524741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79.109161</td>\n",
       "      <td>84.299133</td>\n",
       "      <td>89.786652</td>\n",
       "      <td>91.955315</td>\n",
       "      <td>95.080559</td>\n",
       "      <td>96.521202</td>\n",
       "      <td>97.841942</td>\n",
       "      <td>98.814957</td>\n",
       "      <td>101.628532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             q_0.1        q_0.2        q_0.3        q_0.4        q_0.5  \\\n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  7776.000000   \n",
       "mean      9.321143    12.641466    15.259423    17.068478    18.518168   \n",
       "std      15.374304    19.111445    21.897484    23.809573    25.310299   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.242156   \n",
       "75%      13.262892    20.672598    26.426271    30.780162    34.629218   \n",
       "max      79.109161    84.299133    89.786652    91.955315    95.080559   \n",
       "\n",
       "             q_0.6        q_0.7        q_0.8        q_0.9  \n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  \n",
       "mean     19.768997    20.917525    22.095411    23.340250  \n",
       "std      26.574215    27.706749    28.869001    30.088685  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.757101     1.285631     1.912275     2.569417  \n",
       "75%      37.537903    40.414082    43.398237    46.524741  \n",
       "max      96.521202    97.841942    98.814957   101.628532  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission_v1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "dacon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
